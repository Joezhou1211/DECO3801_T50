{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:17:57.591736Z",
     "start_time": "2024-08-04T14:08:54.268694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import bson\n",
    "\n",
    "#      ===== è¯»å–æ•°æ® ===== \n",
    "# read the data\n",
    "with open('../../data/raw/v_forest.bson', 'rb') as file:\n",
    "    data = bson.decode_all(file.read())\n",
    "\n",
    "# convert the data to a pandas dataframe\n",
    "df = pd.DataFrame(data)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:17:57.714593Z",
     "start_time": "2024-08-04T14:17:57.645101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# å¤‡ä»½ä¸€ä¸ªdf_backup  å¯ä»¥ä½¿ç”¨df = df_backup.copy()æ¥æ¢å¤åŽŸå§‹æ•°æ®     ===== å¤‡ä»½ =====\n",
    "# åŽæœŸæœ‰ä¸€äº›éœ€è¦ä¿®æ”¹æ•°æ®çš„åœ°æ–¹ æ·±æ‹·è´ä¸€ä¸ªå¤‡ä»½ æ–¹ä¾¿å›žæº¯æ•°æ® ä¸ç”¨å†åŠ è½½ä¸€æ¬¡\n",
    "# df_backup = df.copy() "
   ],
   "id": "5a862c6973db204c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:17:57.720123Z",
     "start_time": "2024-08-04T14:17:57.715293Z"
    }
   },
   "cell_type": "code",
   "source": "# df.head()  # Cehck structure of the data     ===== æ•°æ®ç»“æž„ ===== ",
   "id": "cb2553407ee04834",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:17:57.722868Z",
     "start_time": "2024-08-04T14:17:57.721119Z"
    }
   },
   "cell_type": "code",
   "source": "# df.info()   # æ­¤æ—¶æ•°æ®åº”è¯¥æ˜¯(490653,43) ===== æ•°æ®ç±»åž‹ ===== ",
   "id": "4706b195df8bd588",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:19:23.290580Z",
     "start_time": "2024-08-04T14:17:57.725627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Only keep the English tweets     ===== è¯­è¨€ ===== Verified in MongoDB CompassðŸ‘Œ\n",
    "df = df[df['lang'] == 'en']\n",
    "print(df.shape)  # è¡¨ç¤º400174è¡Œ, 43åˆ—"
   ],
   "id": "eb2a00cb94c4273d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400174, 43)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:20:07.289385Z",
     "start_time": "2024-08-04T14:19:23.296874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æ˜¾ç¤ºæŽ¨æ–‡æ—¶é—´çš„æœ€æ—©å’Œæœ€æ™šæ—¶é—´    ===== æ—¶é—´ =====  Verified in MongoDB CompassðŸ‘Œ\n",
    "print(df['created_at_dt'].min()) \n",
    "# min = 2009-09-05 23:06:34\n",
    "print(df['created_at_dt'].max())\n",
    "# max = 2020-02-19 18:42:10\n",
    "\n",
    "# æ ¹æ®é¢˜æ„ æŸ¥è¯¢äº†æ”¿åºœç½‘ç«™å¯¹2019-2020å±±ç«çš„æ—¶é—´æ¡†æž¶ å±±ç«æ—¶é—´ä¸ºJuly 2019 to late February 2020 å‡ºäºŽä¸¥è°¨è€ƒè™‘ æ—¶é—´æ¡†æž¶å‘å‰æŽ¨è‡³2019-04-01 ç”±äºŽå¯èƒ½çš„æƒ…ç»ªè”“å»¶ æ¡†æž¶å‘åŽæŽ¨è‡³2020-6-1 \n",
    "# æ³¨æ„ï¼šæ ¹æ®æ­¤è€ƒé‡ æ­¤æ—¶é—´èŠ‚ç‚¹åŽçš„æŽ¨æ–‡ å¾ˆå¯èƒ½æ˜¯éž­å°¸çš„å‡æ–°é—» \n",
    "# ref: https://www.aph.gov.au/About_Parliament/Parliamentary_Departments/Parliamentary_Library/pubs/rp/rp2122/201920AustralianBushfiresFAQupdate#_ftn1\n",
    "start_date = pd.Timestamp('2019-04-01')\n",
    "end_date = pd.Timestamp('2020-6-1')\n",
    "\n",
    "mask = (df['created_at_dt'] >= start_date) & (df['created_at_dt'] <= end_date)\n",
    "df = df.loc[mask]\n",
    "print(df.shape)  # (397726, 43)"
   ],
   "id": "9c45e17edd2f9679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-05 23:06:34\n",
      "2020-02-19 18:42:10\n",
      "(397726, 43)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:20:41.656966Z",
     "start_time": "2024-08-04T14:20:07.292669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# å¿½ç•¥è½¬å‘/å›žå¤çš„æŽ¨æ–‡ åªä½¿ç”¨çˆ¶æŽ¨æ–‡ df['parent'] = null       ===== çˆ¶æŽ¨æ–‡ =====  Verified in MongoDB CompassðŸ‘Œ\n",
    "df = df[df['parent'].isnull()]\n",
    "\n",
    "print(df.shape)  # (190316, 43)"
   ],
   "id": "c33b35f442b04ec2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190316, 43)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:20:45.363553Z",
     "start_time": "2024-08-04T14:20:41.659800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# åŠ è½½é¢„å¤„ç†èµ„æº ä¸Žæ•°æ®å¤„ç†æœ¬ä½“åˆ†ç¦» ä¾¿äºŽåŽæœŸè°ƒæ•´æ•°æ®å¤„ç†æµç¨‹\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# ä¸‹è½½å¿…è¦çš„NLTKèµ„æº\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ],
   "id": "23373497fee4a86c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhoujingfeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhoujingfeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:20:52.107891Z",
     "start_time": "2024-08-04T14:20:45.364839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# é¢„å¤„ç†å‡½æ•° ===== é¢„å¤„ç† =====  \n",
    "# ç”±äºŽregexçš„è¿ä½œæ–¹æ³•åœ¨pythonå’Œmongodbä¸­ä¸åŒ æ­¤æ­¥éª¤æ— æ³•è¿›è¡ŒéªŒè¯ æ¸…å„ä½åŒä»ä»”ç»†æ£€æŸ¥ä»£ç å’Œè¾“å‡º\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # è½¬æ¢ä¸ºå°å†™\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # ç§»é™¤URL\n",
    "    text = re.sub(r'\\W', ' ', text)  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦\n",
    "    text = text.split()  # åˆ†è¯\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]  # è¯å½¢è¿˜åŽŸå’ŒåŽ»é™¤åœç”¨è¯\n",
    "    text = [word.strip() for word in text] # åŽ»é™¤æ–‡æœ¬å‰åŽç©ºæ ¼\n",
    "    return ' '.join(text)\n",
    "\n",
    "# ä¿å­˜æ–‡æœ¬åˆ°æ–°åˆ—\n",
    "df['cleaned_text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# æŸ¥çœ‹é¢„å¤„ç†åŽçš„æ•°æ®\n",
    "df[['text', 'cleaned_text']].head(10)"
   ],
   "id": "c56845e7d530c4cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   text  \\\n",
       "3159  https://t.co/9wQlvxcHcD                     Ro...   \n",
       "3169  I'M SCREAMING!! ðŸ˜­ðŸ˜­ðŸ¤£ðŸ¤£\\nY'all need to see this ðŸ˜­...   \n",
       "3195  ðŸ—£ Seriously. It's never been this easy to crea...   \n",
       "3234  This morning @ViktoriaRusso, @PatriciaAlves &a...   \n",
       "3241  It was wonderful to welcome @KatyRobertson to ...   \n",
       "3265  Nearly half of Australian children donâ€™t think...   \n",
       "3323  .@HayleyDrubin gives us a visual representatio...   \n",
       "3335  Australia, where people can tether baby dugong...   \n",
       "3341  #PoliticsofCoal @AndreaDeschamps \\nPart 1: Aus...   \n",
       "3350  Sport brings communities together. It also hel...   \n",
       "\n",
       "                                           cleaned_text  \n",
       "3159  rock roll dream angelina jolie bad rock roll d...  \n",
       "3169                                 screaming need see  \n",
       "3195  seriously never easy create great marketing video  \n",
       "3234  morning viktoriarusso patriciaalves amp announ...  \n",
       "3241  wonderful welcome katyrobertson mosman rowing ...  \n",
       "3265  nearly half australian child think bread amp c...  \n",
       "3323  hayleydrubin give u visual representation carb...  \n",
       "3335  australia people tether baby dugong use bait l...  \n",
       "3341  politicsofcoal andreadeschamps part 1 australi...  \n",
       "3350  sport brings community together also help keep...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>https://t.co/9wQlvxcHcD                     Ro...</td>\n",
       "      <td>rock roll dream angelina jolie bad rock roll d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>I'M SCREAMING!! ðŸ˜­ðŸ˜­ðŸ¤£ðŸ¤£\\nY'all need to see this ðŸ˜­...</td>\n",
       "      <td>screaming need see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>ðŸ—£ Seriously. It's never been this easy to crea...</td>\n",
       "      <td>seriously never easy create great marketing video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3234</th>\n",
       "      <td>This morning @ViktoriaRusso, @PatriciaAlves &amp;a...</td>\n",
       "      <td>morning viktoriarusso patriciaalves amp announ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>It was wonderful to welcome @KatyRobertson to ...</td>\n",
       "      <td>wonderful welcome katyrobertson mosman rowing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>Nearly half of Australian children donâ€™t think...</td>\n",
       "      <td>nearly half australian child think bread amp c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>.@HayleyDrubin gives us a visual representatio...</td>\n",
       "      <td>hayleydrubin give u visual representation carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>Australia, where people can tether baby dugong...</td>\n",
       "      <td>australia people tether baby dugong use bait l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>#PoliticsofCoal @AndreaDeschamps \\nPart 1: Aus...</td>\n",
       "      <td>politicsofcoal andreadeschamps part 1 australi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>Sport brings communities together. It also hel...</td>\n",
       "      <td>sport brings community together also help keep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:21:29.624936Z",
     "start_time": "2024-08-04T14:20:52.108580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# è¿‡æ»¤æŽ‰çº¯å¹¿å‘ŠæŽ¨æ–‡ ä½†ç•™ä¸‹ä»¥å±±ç«ä¸ºé¢˜æ‰“å¹¿å‘Šçš„æŽ¨æ–‡  ===== å¹¿å‘Š ===== \n",
    "# å®šä¹‰å¹¿å‘Šå…³é”®è¯å’Œå±±ç«å…³é”®è¯\n",
    "ad_keywords = ['buy now', 'sale', 'discount', 'free shipping', 'deals', 'promotion', \n",
    "               'special offer', 'clearance', 'flash sale', 'exclusive deal', 'shop now', \n",
    "               'buy one get one free', 'best price', 'hot item', 'online store', 'save big', \n",
    "               'coupon code', 'percent off', 'markdown', 'bargain', 'hot deal', 'limited offer', \n",
    "               'sale ends soon', 'big savings', 'exclusive offer', 'limited stock', 'order now', \n",
    "               'act fast', 'free gift', 'lowest price', 'best deal', 'special price', 'hot sale', \n",
    "               'mega sale', 'discount code']\n",
    "\n",
    "fire_keywords = ['burns', 'fire', 'dangerous', 'burn', 'burning', 'died', 'saved', 'bushfire', \n",
    "                   'wildfire', 'forest fire', 'flames', 'evacuate', 'evacuation', 'firefighter', \n",
    "                   'smoke', 'ash', 'firestorm', 'emergency', 'hazard', 'fire season', 'blaze', \n",
    "                   'scorching', 'inferno', 'emergency response', 'climate crisis', \n",
    "                   'climate emergency', 'natural disaster', 'red alert', 'alert', 'containment', \n",
    "                   'fire suppression', 'firefighting', 'wildfire season']\n",
    "\n",
    "# æž„å»ºæ­£åˆ™è¡¨è¾¾å¼\n",
    "ad_pattern = r'\\b(?:' + '|'.join(ad_keywords) + r')\\b'   \n",
    "fire_pattern = r'\\b(?:' + '|'.join(fire_keywords) + r')\\b'\n",
    "\n",
    "# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œå¸ƒå°”ç´¢å¼•\n",
    "ad_mask = df['cleaned_text'].str.contains(ad_pattern)\n",
    "non_ad_mask = df['cleaned_text'].str.contains(fire_pattern)\n",
    "\n",
    "# ä½¿ç”¨å¸ƒå°”ç´¢å¼•è¿‡æ»¤æ•°æ®\n",
    "filtered_df = df[ad_mask & ~non_ad_mask]  # ä¸Žéžæ“ä½œ æ‰¾åˆ°ä¸æ˜¯å…³äºŽå±±ç«çš„å¹¿å‘Š\n",
    "\n",
    "# ä»ŽåŽŸå§‹æ•°æ®ä¸­åˆ é™¤å¹¿å‘ŠæŽ¨æ–‡\n",
    "df = df[~df.index.isin(filtered_df.index)]\n",
    "\n",
    "print(df.shape)   # è¿‡æ»¤æŽ‰äº†(493, 44)æ¡å¹¿å‘ŠæŽ¨æ–‡ å‰©ä½™ï¼ˆ189823, 44ï¼‰æ¡æŽ¨æ–‡\n"
   ],
   "id": "a5be05f209eb1980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189823, 44)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:22:07.651446Z",
     "start_time": "2024-08-04T14:21:31.013087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“\n",
    "from gensim import corpora, models\n",
    "\n",
    "# åˆ›å»ºè¯å…¸: è¯å…¸æ˜¯ä¸€ä¸ªå°†å•è¯æ˜ å°„åˆ°æ•´æ•°idçš„æ˜ å°„ æ¯ä¸€ä¸ªå•è¯éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„id ç”¨äºŽåˆ›å»ºè¯­æ–™åº“\n",
    "dictionary = corpora.Dictionary([tweet.split() for tweet in df['cleaned_text']])\n",
    "\n",
    "# åˆ›å»ºè¯­æ–™åº“: è¯­æ–™åº“æ˜¯ä¸€ä¸ªå°†æ–‡æ¡£è½¬æ¢ä¸ºè¯è¢‹è¡¨ç¤ºçš„å¯¹è±¡ è¯è¢‹æ˜¯ä¸€ä¸ªç¨€ç–å‘é‡ å…¶ä¸­æ¯ä¸ªå•è¯çš„idæ˜ å°„åˆ°å…¶åœ¨æ–‡æ¡£ä¸­çš„å‡ºçŽ°æ¬¡æ•° \n",
    "corpus = [dictionary.doc2bow(tweet.split()) for tweet in df['cleaned_text']]"
   ],
   "id": "a74268d684d745b7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:22:07.659781Z",
     "start_time": "2024-08-04T14:22:07.654719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "# ä¸»é¢˜æ¨¡åž‹  ===== LDA =====  æ­¤ä»»åŠ¡æ— æ³•åœ¨Jupyterä¸­å¹¶å‘  åœ¨Mac M1ä¸Šè¿è¡Œæ—¶é•¿çº¦1.5å°æ—¶å·¦å³ (å‚æ•°[2,30,2]) éªŒè¯å¯è¾“å‡ºä»£ç å—10ä¹‹åŽçš„csvæ–‡ä»¶ ç”¨.pyæ ¼å¼è¿è¡Œå¤šæ ¸å¹¶å‘LDAå»ºæ¨¡\n",
    "# ref:https://wenku.csdn.net/column/68rabmq8w3#1.1%20 å®Œæ•´LDAä»‹ç» \n",
    "# ref:https://docs.pingcode.com/ask/174423.html å‚æ•°è§£é‡Š\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def train_lda_and_evaluate(num_topics):\n",
    "    model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=[tweet.split() for tweet in df['cleaned_text']], dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherencemodel.get_coherence()\n",
    "    perplexity = model.log_perplexity(corpus)\n",
    "    return num_topics, coherence, perplexity\n",
    "\n",
    "# è°ƒå‚ å¯»æ‰¾æœ€ä½³ä¸»é¢˜æ•°K\n",
    "start = 2\n",
    "limit = 30\n",
    "step = 2\n",
    "\n",
    "best_num_topics = start\n",
    "best_coherence = 0\n",
    "\n",
    "for num_topics in range(start, limit + 1, step):\n",
    "    num_topics, coherence, perplexity = train_lda_and_evaluate(num_topics)\n",
    "    print(f\"Number of Topics: {num_topics} \\t Coherence Score: {coherence} \\t Perplexity: {perplexity}\")\n",
    "    if coherence > best_coherence:\n",
    "        best_coherence = coherence\n",
    "        best_num_topics = num_topics\n",
    "\n",
    "print(f\"Best number of topics: {best_num_topics}\")  # ä¸€è‡´æ€§åˆ†æ•°\n",
    "print(f\"Best Coherence Score: {best_coherence}\")  # å›°æƒ‘åº¦\n",
    "\n",
    "# Number of Topics: 2 \t Coherence Score: 0.3759011059947136 \t Perplexity: -8.296418882976667\n",
    "# Number of Topics: 4 \t Coherence Score: 0.42184629557748243 \t Perplexity: -8.299548671751104\n",
    "# Number of Topics: 6 \t Coherence Score: 0.43786540809139907 \t Perplexity: -8.510174269697803\n",
    "# Number of Topics: 8 \t Coherence Score: 0.47176915985311024 \t Perplexity: -8.74394636272138\n",
    "# Number of Topics: 10 \t Coherence Score: 0.4738715046477873 \t Perplexity: -9.159282468691316\n",
    "# Number of Topics: 12 \t Coherence Score: 0.43104690782377725 \t Perplexity: -9.940450411910698\n",
    "# Number of Topics: 14 \t Coherence Score: 0.41125086867564276 \t Perplexity: -10.647159945542178\n",
    "# Number of Topics: 16 \t Coherence Score: 0.4120749905900342 \t Perplexity: -11.182660855673397\n",
    "# Number of Topics: 18 \t Coherence Score: 0.4403819680385084 \t Perplexity: -11.831301903587304\n",
    "# Number of Topics: 20 \t Coherence Score: 0.4282543571789759 \t Perplexity: -12.212385516719994\n",
    "# Number of Topics: 22 \t Coherence Score: 0.42327749443398766 \t Perplexity: -12.60193505708384\n",
    "# Number of Topics: 24 \t Coherence Score: 0.4176014410867455 \t Perplexity: -12.991686981669742\n",
    "# Number of Topics: 26 \t Coherence Score: 0.3966323119642199 \t Perplexity: -13.359620649761812\n",
    "# Number of Topics: 28 \t Coherence Score: 0.38242442503126844 \t Perplexity: -13.824911991294304\n",
    "# Number of Topics: 30 \t Coherence Score: 0.4018442087621527 \t Perplexity: -14.1170189810078\n",
    "# Best number of topics: 10\n",
    "# Best Coherence Score: 0.4738715046477873\n",
    "'''\n"
   ],
   "id": "88f4d1f05bbef6fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ä¸»é¢˜æ¨¡åž‹  ===== LDA =====  æ­¤ä»»åŠ¡æ— æ³•åœ¨Jupyterä¸­å¹¶å‘  åœ¨Mac M1ä¸Šè¿è¡Œæ—¶é•¿çº¦1.5å°æ—¶å·¦å³ (å‚æ•°[2,30,2]) éªŒè¯å¯è¾“å‡ºä»£ç å—10ä¹‹åŽçš„csvæ–‡ä»¶ ç”¨.pyæ ¼å¼è¿è¡Œå¤šæ ¸å¹¶å‘LDAå»ºæ¨¡\\n# ref:https://wenku.csdn.net/column/68rabmq8w3#1.1%20 å®Œæ•´LDAä»‹ç» \\n# ref:https://docs.pingcode.com/ask/174423.html å‚æ•°è§£é‡Š\\nfrom gensim.models import CoherenceModel\\n\\ndef train_lda_and_evaluate(num_topics):\\n    model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\\n    coherencemodel = CoherenceModel(model=model, texts=[tweet.split() for tweet in df[\\'cleaned_text\\']], dictionary=dictionary, coherence=\\'c_v\\')\\n    coherence = coherencemodel.get_coherence()\\n    perplexity = model.log_perplexity(corpus)\\n    return num_topics, coherence, perplexity\\n\\n# è°ƒå‚ å¯»æ‰¾æœ€ä½³ä¸»é¢˜æ•°K\\nstart = 2\\nlimit = 30\\nstep = 2\\n\\nbest_num_topics = start\\nbest_coherence = 0\\n\\nfor num_topics in range(start, limit + 1, step):\\n    num_topics, coherence, perplexity = train_lda_and_evaluate(num_topics)\\n    print(f\"Number of Topics: {num_topics} \\t Coherence Score: {coherence} \\t Perplexity: {perplexity}\")\\n    if coherence > best_coherence:\\n        best_coherence = coherence\\n        best_num_topics = num_topics\\n\\nprint(f\"Best number of topics: {best_num_topics}\")  # ä¸€è‡´æ€§åˆ†æ•°\\nprint(f\"Best Coherence Score: {best_coherence}\")  # å›°æƒ‘åº¦\\n\\n# Number of Topics: 2 \\t Coherence Score: 0.3759011059947136 \\t Perplexity: -8.296418882976667\\n# Number of Topics: 4 \\t Coherence Score: 0.42184629557748243 \\t Perplexity: -8.299548671751104\\n# Number of Topics: 6 \\t Coherence Score: 0.43786540809139907 \\t Perplexity: -8.510174269697803\\n# Number of Topics: 8 \\t Coherence Score: 0.47176915985311024 \\t Perplexity: -8.74394636272138\\n# Number of Topics: 10 \\t Coherence Score: 0.4738715046477873 \\t Perplexity: -9.159282468691316\\n# Number of Topics: 12 \\t Coherence Score: 0.43104690782377725 \\t Perplexity: -9.940450411910698\\n# Number of Topics: 14 \\t Coherence Score: 0.41125086867564276 \\t Perplexity: -10.647159945542178\\n# Number of Topics: 16 \\t Coherence Score: 0.4120749905900342 \\t Perplexity: -11.182660855673397\\n# Number of Topics: 18 \\t Coherence Score: 0.4403819680385084 \\t Perplexity: -11.831301903587304\\n# Number of Topics: 20 \\t Coherence Score: 0.4282543571789759 \\t Perplexity: -12.212385516719994\\n# Number of Topics: 22 \\t Coherence Score: 0.42327749443398766 \\t Perplexity: -12.60193505708384\\n# Number of Topics: 24 \\t Coherence Score: 0.4176014410867455 \\t Perplexity: -12.991686981669742\\n# Number of Topics: 26 \\t Coherence Score: 0.3966323119642199 \\t Perplexity: -13.359620649761812\\n# Number of Topics: 28 \\t Coherence Score: 0.38242442503126844 \\t Perplexity: -13.824911991294304\\n# Number of Topics: 30 \\t Coherence Score: 0.4018442087621527 \\t Perplexity: -14.1170189810078\\n# Best number of topics: 10\\n# Best Coherence Score: 0.4738715046477873\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "# LDAè°ƒå‚ æ‰¾åˆ°passes å’Œiterations  ===== LDA =====\n",
    "# ï¼ï¼ä¸è¦è¿è¡Œè¿™æ®µä»£ç  è¦è·‘8å°æ—¶ä»¥ä¸Š\n",
    "\n",
    "# å®šä¹‰å‚æ•°æœç´¢ç©ºé—´\n",
    "passes_list = [20, 40, 60, 80]\n",
    "iterations_list = [12, 14, 16]\n",
    "\n",
    "# åˆå§‹åŒ–å˜é‡ä»¥å­˜å‚¨æœ€ä½³å‚æ•°\n",
    "best_coherence = -1\n",
    "best_perplexity = float('inf')\n",
    "best_passes = None\n",
    "best_iterations = None\n",
    "\n",
    "# è¿›è¡Œå‚æ•°æœç´¢\n",
    "for passes in passes_list:\n",
    "    for iterations in iterations_list:\n",
    "        # è®­ç»ƒLDAæ¨¡åž‹\n",
    "        lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=passes, iterations=iterations, eval_every=1)\n",
    "        \n",
    "        # è®¡ç®—å›°æƒ‘åº¦\n",
    "        perplexity = lda_model.log_perplexity(corpus)\n",
    "        \n",
    "        # è®¡ç®—ä¸»é¢˜ä¸€è‡´æ€§\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=[tweet.split() for tweet in df['cleaned_text']], dictionary=dictionary, coherence='c_v')\n",
    "        coherence = coherence_model_lda.get_coherence()\n",
    "        \n",
    "        # è¾“å‡ºå½“å‰å‚æ•°çš„è¯„ä¼°ç»“æžœ\n",
    "        print(f\"Passes: {passes}, Iterations: {iterations}, Coherence: {coherence}, Perplexity: {perplexity}\")\n",
    "        \n",
    "        # æ›´æ–°æœ€ä½³å‚æ•°\n",
    "        if coherence > best_coherence and perplexity < best_perplexity:\n",
    "            best_coherence = coherence\n",
    "            best_perplexity = perplexity\n",
    "            best_passes = passes\n",
    "            best_iterations = iterations\n",
    "\n",
    "print(f\"Best Passes: {best_passes}, Best Iterations: {best_iterations}, Best Coherence: {best_coherence}, Best Perplexity: {best_perplexity}\")\n",
    "\n",
    "# Passes: 20, Iterations: 12, Coherence: 0.45824873948325983, Perplexity: -9.03431114214111\n",
    "# Passes: 20, Iterations: 14, Coherence: 0.44837592847592934, Perplexity: -9.12432583705112\n",
    "# Passes: 20, Iterations: 16, Coherence: 0.48747396284574838, Perplexity: -9.15307253900221   ** é€‰å®š **\n",
    "# Passes: 40, Iterations: 12, Coherence: 0.48289085159458595, Perplexity: -9.083461884051694 \n",
    "# Passes: 40, Iterations: 14, Coherence: 0.4273950074677269, Perplexity: -9.082347179210101\n",
    "# Passes: 40, Iterations: 16, Coherence: 0.4712308458087272, Perplexity: -9.090992752020139\n",
    "# Passes: 60, Iterations: 14, Coherence: 0.4525172695340438, Perplexity: -9.071471253067294\n",
    "# Passes: 60, Iterations: 16, Coherence: 0.47780339081003687, Perplexity: -9.11608313556567\n",
    "# Passes: 80, Iterations: 12, Coherence: 0.4611642988790317, Perplexity: -9.060177368492962\n",
    "'''"
   ],
   "id": "df6594484479d92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# LDAè°ƒå‚ æ‰¾åˆ°passes å’Œiterations  ===== LDA =====\\n# ï¼ï¼ä¸è¦è¿è¡Œè¿™æ®µä»£ç  è¦è·‘8å°æ—¶ä»¥ä¸Š\\n\\n# å®šä¹‰å‚æ•°æœç´¢ç©ºé—´\\npasses_list = [20, 40, 60, 80]\\niterations_list = [12, 14, 16]\\n\\n# åˆå§‹åŒ–å˜é‡ä»¥å­˜å‚¨æœ€ä½³å‚æ•°\\nbest_coherence = -1\\nbest_perplexity = float(\\'inf\\')\\nbest_passes = None\\nbest_iterations = None\\n\\n# è¿›è¡Œå‚æ•°æœç´¢\\nfor passes in passes_list:\\n    for iterations in iterations_list:\\n        # è®­ç»ƒLDAæ¨¡åž‹\\n        lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=passes, iterations=iterations, eval_every=1)\\n        \\n        # è®¡ç®—å›°æƒ‘åº¦\\n        perplexity = lda_model.log_perplexity(corpus)\\n        \\n        # è®¡ç®—ä¸»é¢˜ä¸€è‡´æ€§\\n        coherence_model_lda = CoherenceModel(model=lda_model, texts=[tweet.split() for tweet in df[\\'cleaned_text\\']], dictionary=dictionary, coherence=\\'c_v\\')\\n        coherence = coherence_model_lda.get_coherence()\\n        \\n        # è¾“å‡ºå½“å‰å‚æ•°çš„è¯„ä¼°ç»“æžœ\\n        print(f\"Passes: {passes}, Iterations: {iterations}, Coherence: {coherence}, Perplexity: {perplexity}\")\\n        \\n        # æ›´æ–°æœ€ä½³å‚æ•°\\n        if coherence > best_coherence and perplexity < best_perplexity:\\n            best_coherence = coherence\\n            best_perplexity = perplexity\\n            best_passes = passes\\n            best_iterations = iterations\\n\\nprint(f\"Best Passes: {best_passes}, Best Iterations: {best_iterations}, Best Coherence: {best_coherence}, Best Perplexity: {best_perplexity}\")\\n\\n# Passes: 20, Iterations: 12, Coherence: 0.45824873948325983, Perplexity: -9.03431114214111\\n# Passes: 20, Iterations: 14, Coherence: 0.44837592847592934, Perplexity: -9.12432583705112\\n# Passes: 20, Iterations: 16, Coherence: 0.48747396284574838, Perplexity: -9.15307253900221   ** é€‰å®š **\\n# Passes: 40, Iterations: 12, Coherence: 0.48289085159458595, Perplexity: -9.083461884051694 \\n# Passes: 40, Iterations: 14, Coherence: 0.4273950074677269, Perplexity: -9.082347179210101\\n# Passes: 40, Iterations: 16, Coherence: 0.4712308458087272, Perplexity: -9.090992752020139\\n# Passes: 60, Iterations: 14, Coherence: 0.4525172695340438, Perplexity: -9.071471253067294\\n# Passes: 60, Iterations: 16, Coherence: 0.47780339081003687, Perplexity: -9.11608313556567\\n# Passes: 80, Iterations: 12, Coherence: 0.4611642988790317, Perplexity: -9.060177368492962\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:26:46.659993Z",
     "start_time": "2024-08-04T14:22:07.666487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆLDAæ¨¡åž‹\n",
    "final_lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20, iterations=16, eval_every=180000)\n",
    "\n",
    "# è¾“å‡ºæœ€ç»ˆæ¨¡åž‹çš„ä¸»é¢˜\n",
    "for idx, topic in final_lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx}\\nWords: {topic}\\n\")\n",
    "    \n",
    "'''\n",
    "Topic: 0\n",
    "Words: 0.032*\"1\" + 0.027*\"year\" + 0.025*\"2\" + 0.024*\"million\" + 0.022*\"000\" + 0.020*\"billion\" + 0.020*\"animal\" + 0.018*\"3\" + 0.017*\"5\" + 0.014*\"4\"\n",
    "\n",
    "Topic: 1\n",
    "Words: 0.027*\"bushfire\" + 0.027*\"vicfires\" + 0.018*\"info\" + 0.017*\"fire\" + 0.015*\"smoke\" + 0.014*\"south\" + 0.013*\"amp\" + 0.013*\"rain\" + 0.012*\"advice\" + 0.011*\"air\"\n",
    "\n",
    "Topic: 2\n",
    "Words: 0.089*\"australia\" + 0.051*\"koala\" + 0.039*\"australiabushfires\" + 0.036*\"australiaburning\" + 0.035*\"australiaonfire\" + 0.030*\"animal\" + 0.030*\"australiafires\" + 0.028*\"australianbushfiresdisaster\" + 0.020*\"australianfires\" + 0.017*\"rain\"\n",
    "\n",
    "Topic: 3\n",
    "Words: 0.019*\"like\" + 0.016*\"people\" + 0.014*\"know\" + 0.013*\"get\" + 0.012*\"one\" + 0.010*\"look\" + 0.010*\"right\" + 0.010*\"would\" + 0.010*\"think\" + 0.009*\"need\"\n",
    "\n",
    "Topic: 4\n",
    "Words: 0.067*\"auspol\" + 0.032*\"scottmorrisonmp\" + 0.019*\"australiaburns\" + 0.017*\"scottyfrommarketing\" + 0.017*\"government\" + 0.015*\"climateemergency\" + 0.014*\"morrison\" + 0.013*\"amp\" + 0.011*\"australian\" + 0.011*\"australianbushfiredisaster\"\n",
    "\n",
    "Topic: 5\n",
    "Words: 0.063*\"climate\" + 0.041*\"change\" + 0.017*\"bushfireaustralia\" + 0.014*\"amp\" + 0.008*\"climatechange\" + 0.007*\"science\" + 0.007*\"action\" + 0.007*\"fuel\" + 0.006*\"lie\" + 0.006*\"game\"\n",
    "\n",
    "Topic: 6\n",
    "Words: 0.044*\"help\" + 0.021*\"please\" + 0.020*\"australia\" + 0.017*\"donation\" + 0.016*\"australianbushfires\" + 0.014*\"support\" + 0.014*\"donate\" + 0.014*\"australiafires\" + 0.013*\"australianbushfiredisaster\" + 0.013*\"money\"\n",
    "\n",
    "Topic: 7\n",
    "Words: 0.021*\"thank\" + 0.019*\"firefighter\" + 0.017*\"amp\" + 0.012*\"thanks\" + 0.011*\"home\" + 0.011*\"australianfires\" + 0.011*\"work\" + 0.010*\"volunteer\" + 0.009*\"amazing\" + 0.009*\"people\"\n",
    "\n",
    "Topic: 8\n",
    "Words: 0.044*\"australia\" + 0.026*\"fire\" + 0.017*\"climatechange\" + 0.016*\"australiafires\" + 0.014*\"world\" + 0.014*\"bushfires\" + 0.013*\"climateemergency\" + 0.013*\"australianfires\" + 0.012*\"australianbushfiredisaster\" + 0.011*\"climatecrisis\"\n",
    "\n",
    "Topic: 9\n",
    "Words: 0.037*\"fire\" + 0.015*\"power\" + 0.015*\"nsw\" + 0.014*\"community\" + 0.013*\"water\" + 0.012*\"bushfire\" + 0.011*\"affected\" + 0.011*\"island\" + 0.010*\"service\" + 0.010*\"area\"\n",
    "'''"
   ],
   "id": "285c5eb349ec0d66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.025*\"smoke\" + 0.025*\"bushfiresaustralia\" + 0.024*\"bushfires\" + 0.018*\"bushfirecrisisaustralia\" + 0.018*\"melbourne\" + 0.017*\"australianfires\" + 0.016*\"australia\" + 0.015*\"new\" + 0.014*\"nswfires\" + 0.014*\"australiafires\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.061*\"auspol\" + 0.028*\"scottmorrisonmp\" + 0.022*\"scottyfrommarketing\" + 0.020*\"australiaburns\" + 0.018*\"morrison\" + 0.016*\"australianbushfiredisaster\" + 0.012*\"pm\" + 0.010*\"minister\" + 0.009*\"climateemergency\" + 0.009*\"scott\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.077*\"australia\" + 0.030*\"australiafires\" + 0.028*\"australiaonfire\" + 0.026*\"australiaburning\" + 0.024*\"australianbushfiresdisaster\" + 0.021*\"australiabushfires\" + 0.020*\"fire\" + 0.020*\"australianfires\" + 0.019*\"koala\" + 0.018*\"rain\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.030*\"climate\" + 0.019*\"change\" + 0.018*\"amp\" + 0.018*\"climatechange\" + 0.014*\"australia\" + 0.014*\"auspol\" + 0.012*\"climateemergency\" + 0.011*\"climatecrisis\" + 0.010*\"government\" + 0.009*\"australianbushfiredisaster\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.048*\"info\" + 0.045*\"bushfire\" + 0.042*\"vicfires\" + 0.031*\"amp\" + 0.031*\"watch\" + 0.027*\"advice\" + 0.019*\"act\" + 0.011*\"2020\" + 0.007*\"rainfall\" + 0.007*\"conservation\"\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.031*\"help\" + 0.017*\"support\" + 0.016*\"australianbushfiredisaster\" + 0.016*\"please\" + 0.014*\"donation\" + 0.013*\"australianbushfires\" + 0.013*\"australia\" + 0.012*\"money\" + 0.011*\"donate\" + 0.011*\"australiafires\"\n",
      "\n",
      "Topic: 6\n",
      "Words: 0.045*\"fire\" + 0.015*\"nsw\" + 0.012*\"area\" + 0.012*\"south\" + 0.010*\"bushfire\" + 0.010*\"power\" + 0.010*\"amp\" + 0.009*\"nswfires\" + 0.008*\"storm\" + 0.008*\"rain\"\n",
      "\n",
      "Topic: 7\n",
      "Words: 0.039*\"news\" + 0.019*\"air\" + 0.014*\"good\" + 0.013*\"quality\" + 0.012*\"water\" + 0.011*\"heavy\" + 0.009*\"child\" + 0.009*\"fundraising\" + 0.009*\"canberra\" + 0.008*\"house\"\n",
      "\n",
      "Topic: 8\n",
      "Words: 0.019*\"like\" + 0.015*\"day\" + 0.014*\"one\" + 0.014*\"bushfireaustralia\" + 0.014*\"get\" + 0.013*\"look\" + 0.012*\"well\" + 0.012*\"year\" + 0.012*\"last\" + 0.011*\"time\"\n",
      "\n",
      "Topic: 9\n",
      "Words: 0.030*\"animal\" + 0.023*\"australia\" + 0.018*\"fire\" + 0.018*\"koala\" + 0.016*\"1\" + 0.015*\"million\" + 0.015*\"wildlife\" + 0.014*\"000\" + 0.013*\"billion\" + 0.012*\"home\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTopic: 0\\nWords: 0.032*\"1\" + 0.027*\"year\" + 0.025*\"2\" + 0.024*\"million\" + 0.022*\"000\" + 0.020*\"billion\" + 0.020*\"animal\" + 0.018*\"3\" + 0.017*\"5\" + 0.014*\"4\"\\n\\nTopic: 1\\nWords: 0.027*\"bushfire\" + 0.027*\"vicfires\" + 0.018*\"info\" + 0.017*\"fire\" + 0.015*\"smoke\" + 0.014*\"south\" + 0.013*\"amp\" + 0.013*\"rain\" + 0.012*\"advice\" + 0.011*\"air\"\\n\\nTopic: 2\\nWords: 0.089*\"australia\" + 0.051*\"koala\" + 0.039*\"australiabushfires\" + 0.036*\"australiaburning\" + 0.035*\"australiaonfire\" + 0.030*\"animal\" + 0.030*\"australiafires\" + 0.028*\"australianbushfiresdisaster\" + 0.020*\"australianfires\" + 0.017*\"rain\"\\n\\nTopic: 3\\nWords: 0.019*\"like\" + 0.016*\"people\" + 0.014*\"know\" + 0.013*\"get\" + 0.012*\"one\" + 0.010*\"look\" + 0.010*\"right\" + 0.010*\"would\" + 0.010*\"think\" + 0.009*\"need\"\\n\\nTopic: 4\\nWords: 0.067*\"auspol\" + 0.032*\"scottmorrisonmp\" + 0.019*\"australiaburns\" + 0.017*\"scottyfrommarketing\" + 0.017*\"government\" + 0.015*\"climateemergency\" + 0.014*\"morrison\" + 0.013*\"amp\" + 0.011*\"australian\" + 0.011*\"australianbushfiredisaster\"\\n\\nTopic: 5\\nWords: 0.063*\"climate\" + 0.041*\"change\" + 0.017*\"bushfireaustralia\" + 0.014*\"amp\" + 0.008*\"climatechange\" + 0.007*\"science\" + 0.007*\"action\" + 0.007*\"fuel\" + 0.006*\"lie\" + 0.006*\"game\"\\n\\nTopic: 6\\nWords: 0.044*\"help\" + 0.021*\"please\" + 0.020*\"australia\" + 0.017*\"donation\" + 0.016*\"australianbushfires\" + 0.014*\"support\" + 0.014*\"donate\" + 0.014*\"australiafires\" + 0.013*\"australianbushfiredisaster\" + 0.013*\"money\"\\n\\nTopic: 7\\nWords: 0.021*\"thank\" + 0.019*\"firefighter\" + 0.017*\"amp\" + 0.012*\"thanks\" + 0.011*\"home\" + 0.011*\"australianfires\" + 0.011*\"work\" + 0.010*\"volunteer\" + 0.009*\"amazing\" + 0.009*\"people\"\\n\\nTopic: 8\\nWords: 0.044*\"australia\" + 0.026*\"fire\" + 0.017*\"climatechange\" + 0.016*\"australiafires\" + 0.014*\"world\" + 0.014*\"bushfires\" + 0.013*\"climateemergency\" + 0.013*\"australianfires\" + 0.012*\"australianbushfiredisaster\" + 0.011*\"climatecrisis\"\\n\\nTopic: 9\\nWords: 0.037*\"fire\" + 0.015*\"power\" + 0.015*\"nsw\" + 0.014*\"community\" + 0.013*\"water\" + 0.012*\"bushfire\" + 0.011*\"affected\" + 0.011*\"island\" + 0.010*\"service\" + 0.010*\"area\"\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:28:30.346331Z",
     "start_time": "2024-08-04T14:28:05.269920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# è¾“å‡ºæ¨¡åž‹ åˆ†é…ä¸»é¢˜åˆ°df è¾“å‡ºæ•°æ®åˆ°csvæ–‡ä»¶\n",
    "# åˆ†é…ä¸»é¢˜ \n",
    "def get_dominant_topic(model, corpus):\n",
    "    dominant_topics = []\n",
    "    for bow in corpus:\n",
    "        topic_probs = model.get_document_topics(bow)\n",
    "        dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
    "        dominant_topics.append(dominant_topic)\n",
    "    return dominant_topics\n",
    "\n",
    "df['dominant_topic'] = get_dominant_topic(final_lda_model, corpus)\n"
   ],
   "id": "5d0ca2d7fde77b03",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:31:21.789597Z",
     "start_time": "2024-08-04T14:31:19.098276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# è¾“å‡ºæ¨¡åž‹\n",
    "final_lda_model.save('../../models/lda_model.gensim')\n",
    "dictionary.save('../../data/processed/dictionary.gensim')\n",
    "corpora.MmCorpus.serialize('../../data/processed/corpus.mm', corpus)\n",
    "\n",
    "# ä½¿ç”¨è¿™ä¸ªè¯­æ³•åœ¨jupyteræ¡†æž¶å¤–åŠ è½½æ¨¡åž‹\n",
    "# final_lda_model = models.LdaModel.load('../../models/lda_model.gensim')\n",
    "# dictionary = corpora.Dictionary.load('../../data/processed/dictionary.gensim')\n",
    "# corpus = corpora.MmCorpus('../../data/processed/corpus.mm')"
   ],
   "id": "e1fef9fff2f5e06d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# è¾“å‡ºæ•°æ®åˆ°csvæ–‡ä»¶\n",
    "df.to_csv('../../data/processed/tweets_with_topics.csv', index=False, encoding='utf-8-sig')"
   ],
   "id": "13259145aa239d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ç”Ÿæˆå¯è§†åŒ–HTMLæ–‡ä»¶ä»¥æŸ¥çœ‹å…³é”®topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# å‡†å¤‡pyLDAvisæ•°æ®\n",
    "lda_display = gensimvis.prepare(final_lda_model, corpus, dictionary)\n",
    "\n",
    "# æ˜¾ç¤ºå¯è§†åŒ–ç•Œé¢\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "# ä¿å­˜å¯è§†åŒ–ç»“æžœåˆ°HTMLæ–‡ä»¶\n",
    "pyLDAvis.save_html(lda_display, '../../backend/utils')\n"
   ],
   "id": "44f3e2c055207ea1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ç”Ÿæˆè¯å›¾ æœ‰éœ€è¦æ—¶å¯ä»¥ä½¿ç”¨\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# è‡ªå®šä¹‰å‡½æ•°å°†æ‰€æœ‰è¯æ±‡è½¬æ¢ä¸ºå¤§å†™\n",
    "def to_upper_case(frequencies):\n",
    "    return {word.upper(): freq for word, freq in frequencies.items()}\n",
    "\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªå¤§å›¾ï¼Œå°†æ‰€æœ‰è¯äº‘æ±‡èšåœ¨ä¸€èµ·\n",
    "num_topics = final_lda_model.num_topics\n",
    "fig, axes = plt.subplots(2, (num_topics + 1) // 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "# ç”Ÿæˆæ¯ä¸ªä¸»é¢˜çš„è¯äº‘å¹¶æ±‡èšåˆ°ä¸€å¼ å›¾ä¸­\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < num_topics:\n",
    "        topic_words = dict(final_lda_model.show_topic(i, 200))\n",
    "        topic_words_upper = to_upper_case(topic_words)\n",
    "\n",
    "        wordcloud = WordCloud(width=800, height=600,\n",
    "                              background_color='white',\n",
    "                              max_words=200,\n",
    "                              contour_width=3,\n",
    "                              contour_color='steelblue',\n",
    "                              random_state=21,\n",
    "                              max_font_size=110).generate_from_frequencies(topic_words_upper)\n",
    "\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.set_title(f'Topic {i}', fontsize=16)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "# è°ƒæ•´å­å›¾å¸ƒå±€\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../backend/utils/LDA_wordcloud.png', format='png', bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "740a255838b0f31d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ä»ŽLDA_visualization.htmlæ–‡ä»¶ å’ŒLDA_wordcloud.pngä¸­ æ€»ç»“å‡ºä»¥ä¸‹å’Œå±±ç«ç›¸å…³æ–°é—»å…³é”®è¯\n",
    "# ä½¿ç”¨è¿™äº›å…³é”®è¯å°†æŽ¨æ–‡è¿›è¡Œè¿‡æ»¤ ä¿ç•™ä¸Žå±±ç«ç›¸å…³çš„æŽ¨æ–‡ ä¿å­˜ä¸ºtweets_bushfire_related_keywords.csv\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "df = pd.read_csv('../../data/processed/tweets_with_topics.csv')\n",
    "\n",
    "# From HTML file\n",
    "bushfire_keywords = [ \n",
    "    \"bushfiredisaster\", \"australfires\", \"fire\", \"australianfires\", \"bushfires\",\n",
    "    \"australiaburns\", \"australianbushfiredisaster\", \"australianbushfires\",\n",
    "    \"australfire\", \"australiaburning\", \"koala\", \"animal\", \"australianwildfires\",\n",
    "    \"australianbushfire\", \"firefighter\", \"bushfireaustralia\", \"nswfires\", \"vicfires\",\n",
    "    \"bushfirecrisis\", \"bushfiresaustralia\", \"bushfirecrisisaustralia\"\n",
    "]\n",
    "\n",
    "# ä½¿ç”¨å…³é”®è¯è¿‡æ»¤æŽ¨æ–‡\n",
    "df_bushfire_related = df[\n",
    "    df['text'].str.contains('|'.join(bushfire_keywords), case=False, na=False)\n",
    "]\n",
    "# å°†è¿‡æ»¤åŽçš„æ•°æ®ä¿å­˜åˆ°æ–°çš„CSVæ–‡ä»¶\n",
    "output_path = '../../data/processed/tweets_bushfire_related_keywords.csv'\n",
    "df_bushfire_related.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(len(df_bushfire_related))  # 158902\n"
   ],
   "id": "718d14a4d0223f38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:27:17.689201Z",
     "start_time": "2024-08-04T14:27:14.564891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# èšåˆlocationåˆ—ç»Ÿè®¡æ¯ä¸ªåœ°ç‚¹çš„æŽ¨æ–‡æ•°   ===== åœ°ç‚¹ =====   è¿™æ®µåŽæœŸæœ‰éœ€è¦å†ç”¨\n",
    "location_counts = df.groupby('location').size()\n",
    "location_counts = location_counts.sort_values(ascending=False)\n",
    "print(location_counts.head(10))"
   ],
   "id": "ec44aa84d0cf5933",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "Unknown                       75217\n",
      "Victoria, Australia           18388\n",
      "New South Wales, Australia    18228\n",
      "United States                 15909\n",
      "Australia                     15617\n",
      "United Kingdom                 8312\n",
      "Queensland, Australia          6027\n",
      "South Australia, Australia     4074\n",
      "Canada                         3704\n",
      "India                          3492\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
