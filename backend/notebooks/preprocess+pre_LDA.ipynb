{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:17:57.591736Z",
     "start_time": "2024-08-04T14:08:54.268694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import bson\n",
    "\n",
    "#      ===== 读取数据 ===== \n",
    "# read the data\n",
    "with open('../../data/raw/v_forest.bson', 'rb') as file:\n",
    "    data = bson.decode_all(file.read())\n",
    "\n",
    "# convert the data to a pandas dataframe\n",
    "df = pd.DataFrame(data)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:17:57.714593Z",
     "start_time": "2024-08-04T14:17:57.645101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 备份一个df_backup  可以使用df = df_backup.copy()来恢复原始数据     ===== 备份 =====\n",
    "# 后期有一些需要修改数据的地方 深拷贝一个备份 方便回溯数据 不用再加载一次\n",
    "# df_backup = df.copy() "
   ],
   "id": "5a862c6973db204c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:17:57.720123Z",
     "start_time": "2024-08-04T14:17:57.715293Z"
    }
   },
   "cell_type": "code",
   "source": "# df.head()  # Cehck structure of the data     ===== 数据结构 ===== ",
   "id": "cb2553407ee04834",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:17:57.722868Z",
     "start_time": "2024-08-04T14:17:57.721119Z"
    }
   },
   "cell_type": "code",
   "source": "# df.info()   # 此时数据应该是(490653,43) ===== 数据类型 ===== ",
   "id": "4706b195df8bd588",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:19:23.290580Z",
     "start_time": "2024-08-04T14:17:57.725627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Only keep the English tweets     ===== 语言 ===== Verified in MongoDB Compass👌\n",
    "df = df[df['lang'] == 'en']\n",
    "print(df.shape)  # 表示400174行, 43列"
   ],
   "id": "eb2a00cb94c4273d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400174, 43)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:20:07.289385Z",
     "start_time": "2024-08-04T14:19:23.296874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 显示推文时间的最早和最晚时间    ===== 时间 =====  Verified in MongoDB Compass👌\n",
    "print(df['created_at_dt'].min()) \n",
    "# min = 2009-09-05 23:06:34\n",
    "print(df['created_at_dt'].max())\n",
    "# max = 2020-02-19 18:42:10\n",
    "\n",
    "# 根据题意 查询了政府网站对2019-2020山火的时间框架 山火时间为July 2019 to late February 2020 出于严谨考虑 时间框架向前推至2019-04-01 由于可能的情绪蔓延 框架向后推至2020-6-1 \n",
    "# 注意：根据此考量 此时间节点后的推文 很可能是鞭尸的假新闻 \n",
    "# ref: https://www.aph.gov.au/About_Parliament/Parliamentary_Departments/Parliamentary_Library/pubs/rp/rp2122/201920AustralianBushfiresFAQupdate#_ftn1\n",
    "start_date = pd.Timestamp('2019-04-01')\n",
    "end_date = pd.Timestamp('2020-6-1')\n",
    "\n",
    "mask = (df['created_at_dt'] >= start_date) & (df['created_at_dt'] <= end_date)\n",
    "df = df.loc[mask]\n",
    "print(df.shape)  # (397726, 43)"
   ],
   "id": "9c45e17edd2f9679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-05 23:06:34\n",
      "2020-02-19 18:42:10\n",
      "(397726, 43)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:20:41.656966Z",
     "start_time": "2024-08-04T14:20:07.292669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 忽略转发/回复的推文 只使用父推文 df['parent'] = null       ===== 父推文 =====  Verified in MongoDB Compass👌\n",
    "df = df[df['parent'].isnull()]\n",
    "\n",
    "print(df.shape)  # (190316, 43)"
   ],
   "id": "c33b35f442b04ec2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190316, 43)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:20:45.363553Z",
     "start_time": "2024-08-04T14:20:41.659800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载预处理资源 与数据处理本体分离 便于后期调整数据处理流程\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# 下载必要的NLTK资源\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 数据预处理函数\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ],
   "id": "23373497fee4a86c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhoujingfeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhoujingfeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:20:52.107891Z",
     "start_time": "2024-08-04T14:20:45.364839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 预处理函数 ===== 预处理 =====  \n",
    "# 由于regex的运作方法在python和mongodb中不同 此步骤无法进行验证 清各位同仁仔细检查代码和输出\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # 转换为小写\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # 移除URL\n",
    "    text = re.sub(r'\\W', ' ', text)  # 移除特殊字符\n",
    "    text = text.split()  # 分词\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]  # 词形还原和去除停用词\n",
    "    text = [word.strip() for word in text] # 去除文本前后空格\n",
    "    return ' '.join(text)\n",
    "\n",
    "# 保存文本到新列\n",
    "df['cleaned_text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# 查看预处理后的数据\n",
    "df[['text', 'cleaned_text']].head(10)"
   ],
   "id": "c56845e7d530c4cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   text  \\\n",
       "3159  https://t.co/9wQlvxcHcD                     Ro...   \n",
       "3169  I'M SCREAMING!! 😭😭🤣🤣\\nY'all need to see this 😭...   \n",
       "3195  🗣 Seriously. It's never been this easy to crea...   \n",
       "3234  This morning @ViktoriaRusso, @PatriciaAlves &a...   \n",
       "3241  It was wonderful to welcome @KatyRobertson to ...   \n",
       "3265  Nearly half of Australian children don’t think...   \n",
       "3323  .@HayleyDrubin gives us a visual representatio...   \n",
       "3335  Australia, where people can tether baby dugong...   \n",
       "3341  #PoliticsofCoal @AndreaDeschamps \\nPart 1: Aus...   \n",
       "3350  Sport brings communities together. It also hel...   \n",
       "\n",
       "                                           cleaned_text  \n",
       "3159  rock roll dream angelina jolie bad rock roll d...  \n",
       "3169                                 screaming need see  \n",
       "3195  seriously never easy create great marketing video  \n",
       "3234  morning viktoriarusso patriciaalves amp announ...  \n",
       "3241  wonderful welcome katyrobertson mosman rowing ...  \n",
       "3265  nearly half australian child think bread amp c...  \n",
       "3323  hayleydrubin give u visual representation carb...  \n",
       "3335  australia people tether baby dugong use bait l...  \n",
       "3341  politicsofcoal andreadeschamps part 1 australi...  \n",
       "3350  sport brings community together also help keep...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>https://t.co/9wQlvxcHcD                     Ro...</td>\n",
       "      <td>rock roll dream angelina jolie bad rock roll d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>I'M SCREAMING!! 😭😭🤣🤣\\nY'all need to see this 😭...</td>\n",
       "      <td>screaming need see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>🗣 Seriously. It's never been this easy to crea...</td>\n",
       "      <td>seriously never easy create great marketing video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3234</th>\n",
       "      <td>This morning @ViktoriaRusso, @PatriciaAlves &amp;a...</td>\n",
       "      <td>morning viktoriarusso patriciaalves amp announ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>It was wonderful to welcome @KatyRobertson to ...</td>\n",
       "      <td>wonderful welcome katyrobertson mosman rowing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>Nearly half of Australian children don’t think...</td>\n",
       "      <td>nearly half australian child think bread amp c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>.@HayleyDrubin gives us a visual representatio...</td>\n",
       "      <td>hayleydrubin give u visual representation carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>Australia, where people can tether baby dugong...</td>\n",
       "      <td>australia people tether baby dugong use bait l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>#PoliticsofCoal @AndreaDeschamps \\nPart 1: Aus...</td>\n",
       "      <td>politicsofcoal andreadeschamps part 1 australi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>Sport brings communities together. It also hel...</td>\n",
       "      <td>sport brings community together also help keep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:21:29.624936Z",
     "start_time": "2024-08-04T14:20:52.108580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 过滤掉纯广告推文 但留下以山火为题打广告的推文  ===== 广告 ===== \n",
    "# 定义广告关键词和山火关键词\n",
    "ad_keywords = ['buy now', 'sale', 'discount', 'free shipping', 'deals', 'promotion', \n",
    "               'special offer', 'clearance', 'flash sale', 'exclusive deal', 'shop now', \n",
    "               'buy one get one free', 'best price', 'hot item', 'online store', 'save big', \n",
    "               'coupon code', 'percent off', 'markdown', 'bargain', 'hot deal', 'limited offer', \n",
    "               'sale ends soon', 'big savings', 'exclusive offer', 'limited stock', 'order now', \n",
    "               'act fast', 'free gift', 'lowest price', 'best deal', 'special price', 'hot sale', \n",
    "               'mega sale', 'discount code']\n",
    "\n",
    "fire_keywords = ['burns', 'fire', 'dangerous', 'burn', 'burning', 'died', 'saved', 'bushfire', \n",
    "                   'wildfire', 'forest fire', 'flames', 'evacuate', 'evacuation', 'firefighter', \n",
    "                   'smoke', 'ash', 'firestorm', 'emergency', 'hazard', 'fire season', 'blaze', \n",
    "                   'scorching', 'inferno', 'emergency response', 'climate crisis', \n",
    "                   'climate emergency', 'natural disaster', 'red alert', 'alert', 'containment', \n",
    "                   'fire suppression', 'firefighting', 'wildfire season']\n",
    "\n",
    "# 构建正则表达式\n",
    "ad_pattern = r'\\b(?:' + '|'.join(ad_keywords) + r')\\b'   \n",
    "fire_pattern = r'\\b(?:' + '|'.join(fire_keywords) + r')\\b'\n",
    "\n",
    "# 使用正则表达式进行布尔索引\n",
    "ad_mask = df['cleaned_text'].str.contains(ad_pattern)\n",
    "non_ad_mask = df['cleaned_text'].str.contains(fire_pattern)\n",
    "\n",
    "# 使用布尔索引过滤数据\n",
    "filtered_df = df[ad_mask & ~non_ad_mask]  # 与非操作 找到不是关于山火的广告\n",
    "\n",
    "# 从原始数据中删除广告推文\n",
    "df = df[~df.index.isin(filtered_df.index)]\n",
    "\n",
    "print(df.shape)   # 过滤掉了(493, 44)条广告推文 剩余（189823, 44）条推文\n"
   ],
   "id": "a5be05f209eb1980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189823, 44)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:22:07.651446Z",
     "start_time": "2024-08-04T14:21:31.013087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建词典和语料库\n",
    "from gensim import corpora, models\n",
    "\n",
    "# 创建词典: 词典是一个将单词映射到整数id的映射 每一个单词都有一个唯一的id 用于创建语料库\n",
    "dictionary = corpora.Dictionary([tweet.split() for tweet in df['cleaned_text']])\n",
    "\n",
    "# 创建语料库: 语料库是一个将文档转换为词袋表示的对象 词袋是一个稀疏向量 其中每个单词的id映射到其在文档中的出现次数 \n",
    "corpus = [dictionary.doc2bow(tweet.split()) for tweet in df['cleaned_text']]"
   ],
   "id": "a74268d684d745b7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:22:07.659781Z",
     "start_time": "2024-08-04T14:22:07.654719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "# 主题模型  ===== LDA =====  此任务无法在Jupyter中并发  在Mac M1上运行时长约1.5小时左右 (参数[2,30,2]) 验证可输出代码块10之后的csv文件 用.py格式运行多核并发LDA建模\n",
    "# ref:https://wenku.csdn.net/column/68rabmq8w3#1.1%20 完整LDA介绍 \n",
    "# ref:https://docs.pingcode.com/ask/174423.html 参数解释\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def train_lda_and_evaluate(num_topics):\n",
    "    model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=[tweet.split() for tweet in df['cleaned_text']], dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherencemodel.get_coherence()\n",
    "    perplexity = model.log_perplexity(corpus)\n",
    "    return num_topics, coherence, perplexity\n",
    "\n",
    "# 调参 寻找最佳主题数K\n",
    "start = 2\n",
    "limit = 30\n",
    "step = 2\n",
    "\n",
    "best_num_topics = start\n",
    "best_coherence = 0\n",
    "\n",
    "for num_topics in range(start, limit + 1, step):\n",
    "    num_topics, coherence, perplexity = train_lda_and_evaluate(num_topics)\n",
    "    print(f\"Number of Topics: {num_topics} \\t Coherence Score: {coherence} \\t Perplexity: {perplexity}\")\n",
    "    if coherence > best_coherence:\n",
    "        best_coherence = coherence\n",
    "        best_num_topics = num_topics\n",
    "\n",
    "print(f\"Best number of topics: {best_num_topics}\")  # 一致性分数\n",
    "print(f\"Best Coherence Score: {best_coherence}\")  # 困惑度\n",
    "\n",
    "# Number of Topics: 2 \t Coherence Score: 0.3759011059947136 \t Perplexity: -8.296418882976667\n",
    "# Number of Topics: 4 \t Coherence Score: 0.42184629557748243 \t Perplexity: -8.299548671751104\n",
    "# Number of Topics: 6 \t Coherence Score: 0.43786540809139907 \t Perplexity: -8.510174269697803\n",
    "# Number of Topics: 8 \t Coherence Score: 0.47176915985311024 \t Perplexity: -8.74394636272138\n",
    "# Number of Topics: 10 \t Coherence Score: 0.4738715046477873 \t Perplexity: -9.159282468691316\n",
    "# Number of Topics: 12 \t Coherence Score: 0.43104690782377725 \t Perplexity: -9.940450411910698\n",
    "# Number of Topics: 14 \t Coherence Score: 0.41125086867564276 \t Perplexity: -10.647159945542178\n",
    "# Number of Topics: 16 \t Coherence Score: 0.4120749905900342 \t Perplexity: -11.182660855673397\n",
    "# Number of Topics: 18 \t Coherence Score: 0.4403819680385084 \t Perplexity: -11.831301903587304\n",
    "# Number of Topics: 20 \t Coherence Score: 0.4282543571789759 \t Perplexity: -12.212385516719994\n",
    "# Number of Topics: 22 \t Coherence Score: 0.42327749443398766 \t Perplexity: -12.60193505708384\n",
    "# Number of Topics: 24 \t Coherence Score: 0.4176014410867455 \t Perplexity: -12.991686981669742\n",
    "# Number of Topics: 26 \t Coherence Score: 0.3966323119642199 \t Perplexity: -13.359620649761812\n",
    "# Number of Topics: 28 \t Coherence Score: 0.38242442503126844 \t Perplexity: -13.824911991294304\n",
    "# Number of Topics: 30 \t Coherence Score: 0.4018442087621527 \t Perplexity: -14.1170189810078\n",
    "# Best number of topics: 10\n",
    "# Best Coherence Score: 0.4738715046477873\n",
    "'''\n"
   ],
   "id": "88f4d1f05bbef6fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 主题模型  ===== LDA =====  此任务无法在Jupyter中并发  在Mac M1上运行时长约1.5小时左右 (参数[2,30,2]) 验证可输出代码块10之后的csv文件 用.py格式运行多核并发LDA建模\\n# ref:https://wenku.csdn.net/column/68rabmq8w3#1.1%20 完整LDA介绍 \\n# ref:https://docs.pingcode.com/ask/174423.html 参数解释\\nfrom gensim.models import CoherenceModel\\n\\ndef train_lda_and_evaluate(num_topics):\\n    model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\\n    coherencemodel = CoherenceModel(model=model, texts=[tweet.split() for tweet in df[\\'cleaned_text\\']], dictionary=dictionary, coherence=\\'c_v\\')\\n    coherence = coherencemodel.get_coherence()\\n    perplexity = model.log_perplexity(corpus)\\n    return num_topics, coherence, perplexity\\n\\n# 调参 寻找最佳主题数K\\nstart = 2\\nlimit = 30\\nstep = 2\\n\\nbest_num_topics = start\\nbest_coherence = 0\\n\\nfor num_topics in range(start, limit + 1, step):\\n    num_topics, coherence, perplexity = train_lda_and_evaluate(num_topics)\\n    print(f\"Number of Topics: {num_topics} \\t Coherence Score: {coherence} \\t Perplexity: {perplexity}\")\\n    if coherence > best_coherence:\\n        best_coherence = coherence\\n        best_num_topics = num_topics\\n\\nprint(f\"Best number of topics: {best_num_topics}\")  # 一致性分数\\nprint(f\"Best Coherence Score: {best_coherence}\")  # 困惑度\\n\\n# Number of Topics: 2 \\t Coherence Score: 0.3759011059947136 \\t Perplexity: -8.296418882976667\\n# Number of Topics: 4 \\t Coherence Score: 0.42184629557748243 \\t Perplexity: -8.299548671751104\\n# Number of Topics: 6 \\t Coherence Score: 0.43786540809139907 \\t Perplexity: -8.510174269697803\\n# Number of Topics: 8 \\t Coherence Score: 0.47176915985311024 \\t Perplexity: -8.74394636272138\\n# Number of Topics: 10 \\t Coherence Score: 0.4738715046477873 \\t Perplexity: -9.159282468691316\\n# Number of Topics: 12 \\t Coherence Score: 0.43104690782377725 \\t Perplexity: -9.940450411910698\\n# Number of Topics: 14 \\t Coherence Score: 0.41125086867564276 \\t Perplexity: -10.647159945542178\\n# Number of Topics: 16 \\t Coherence Score: 0.4120749905900342 \\t Perplexity: -11.182660855673397\\n# Number of Topics: 18 \\t Coherence Score: 0.4403819680385084 \\t Perplexity: -11.831301903587304\\n# Number of Topics: 20 \\t Coherence Score: 0.4282543571789759 \\t Perplexity: -12.212385516719994\\n# Number of Topics: 22 \\t Coherence Score: 0.42327749443398766 \\t Perplexity: -12.60193505708384\\n# Number of Topics: 24 \\t Coherence Score: 0.4176014410867455 \\t Perplexity: -12.991686981669742\\n# Number of Topics: 26 \\t Coherence Score: 0.3966323119642199 \\t Perplexity: -13.359620649761812\\n# Number of Topics: 28 \\t Coherence Score: 0.38242442503126844 \\t Perplexity: -13.824911991294304\\n# Number of Topics: 30 \\t Coherence Score: 0.4018442087621527 \\t Perplexity: -14.1170189810078\\n# Best number of topics: 10\\n# Best Coherence Score: 0.4738715046477873\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "# LDA调参 找到passes 和iterations  ===== LDA =====\n",
    "# ！！不要运行这段代码 要跑8小时以上\n",
    "\n",
    "# 定义参数搜索空间\n",
    "passes_list = [20, 40, 60, 80]\n",
    "iterations_list = [12, 14, 16]\n",
    "\n",
    "# 初始化变量以存储最佳参数\n",
    "best_coherence = -1\n",
    "best_perplexity = float('inf')\n",
    "best_passes = None\n",
    "best_iterations = None\n",
    "\n",
    "# 进行参数搜索\n",
    "for passes in passes_list:\n",
    "    for iterations in iterations_list:\n",
    "        # 训练LDA模型\n",
    "        lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=passes, iterations=iterations, eval_every=1)\n",
    "        \n",
    "        # 计算困惑度\n",
    "        perplexity = lda_model.log_perplexity(corpus)\n",
    "        \n",
    "        # 计算主题一致性\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=[tweet.split() for tweet in df['cleaned_text']], dictionary=dictionary, coherence='c_v')\n",
    "        coherence = coherence_model_lda.get_coherence()\n",
    "        \n",
    "        # 输出当前参数的评估结果\n",
    "        print(f\"Passes: {passes}, Iterations: {iterations}, Coherence: {coherence}, Perplexity: {perplexity}\")\n",
    "        \n",
    "        # 更新最佳参数\n",
    "        if coherence > best_coherence and perplexity < best_perplexity:\n",
    "            best_coherence = coherence\n",
    "            best_perplexity = perplexity\n",
    "            best_passes = passes\n",
    "            best_iterations = iterations\n",
    "\n",
    "print(f\"Best Passes: {best_passes}, Best Iterations: {best_iterations}, Best Coherence: {best_coherence}, Best Perplexity: {best_perplexity}\")\n",
    "\n",
    "# Passes: 20, Iterations: 12, Coherence: 0.45824873948325983, Perplexity: -9.03431114214111\n",
    "# Passes: 20, Iterations: 14, Coherence: 0.44837592847592934, Perplexity: -9.12432583705112\n",
    "# Passes: 20, Iterations: 16, Coherence: 0.48747396284574838, Perplexity: -9.15307253900221   ** 选定 **\n",
    "# Passes: 40, Iterations: 12, Coherence: 0.48289085159458595, Perplexity: -9.083461884051694 \n",
    "# Passes: 40, Iterations: 14, Coherence: 0.4273950074677269, Perplexity: -9.082347179210101\n",
    "# Passes: 40, Iterations: 16, Coherence: 0.4712308458087272, Perplexity: -9.090992752020139\n",
    "# Passes: 60, Iterations: 14, Coherence: 0.4525172695340438, Perplexity: -9.071471253067294\n",
    "# Passes: 60, Iterations: 16, Coherence: 0.47780339081003687, Perplexity: -9.11608313556567\n",
    "# Passes: 80, Iterations: 12, Coherence: 0.4611642988790317, Perplexity: -9.060177368492962\n",
    "'''"
   ],
   "id": "df6594484479d92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# LDA调参 找到passes 和iterations  ===== LDA =====\\n# ！！不要运行这段代码 要跑8小时以上\\n\\n# 定义参数搜索空间\\npasses_list = [20, 40, 60, 80]\\niterations_list = [12, 14, 16]\\n\\n# 初始化变量以存储最佳参数\\nbest_coherence = -1\\nbest_perplexity = float(\\'inf\\')\\nbest_passes = None\\nbest_iterations = None\\n\\n# 进行参数搜索\\nfor passes in passes_list:\\n    for iterations in iterations_list:\\n        # 训练LDA模型\\n        lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=passes, iterations=iterations, eval_every=1)\\n        \\n        # 计算困惑度\\n        perplexity = lda_model.log_perplexity(corpus)\\n        \\n        # 计算主题一致性\\n        coherence_model_lda = CoherenceModel(model=lda_model, texts=[tweet.split() for tweet in df[\\'cleaned_text\\']], dictionary=dictionary, coherence=\\'c_v\\')\\n        coherence = coherence_model_lda.get_coherence()\\n        \\n        # 输出当前参数的评估结果\\n        print(f\"Passes: {passes}, Iterations: {iterations}, Coherence: {coherence}, Perplexity: {perplexity}\")\\n        \\n        # 更新最佳参数\\n        if coherence > best_coherence and perplexity < best_perplexity:\\n            best_coherence = coherence\\n            best_perplexity = perplexity\\n            best_passes = passes\\n            best_iterations = iterations\\n\\nprint(f\"Best Passes: {best_passes}, Best Iterations: {best_iterations}, Best Coherence: {best_coherence}, Best Perplexity: {best_perplexity}\")\\n\\n# Passes: 20, Iterations: 12, Coherence: 0.45824873948325983, Perplexity: -9.03431114214111\\n# Passes: 20, Iterations: 14, Coherence: 0.44837592847592934, Perplexity: -9.12432583705112\\n# Passes: 20, Iterations: 16, Coherence: 0.48747396284574838, Perplexity: -9.15307253900221   ** 选定 **\\n# Passes: 40, Iterations: 12, Coherence: 0.48289085159458595, Perplexity: -9.083461884051694 \\n# Passes: 40, Iterations: 14, Coherence: 0.4273950074677269, Perplexity: -9.082347179210101\\n# Passes: 40, Iterations: 16, Coherence: 0.4712308458087272, Perplexity: -9.090992752020139\\n# Passes: 60, Iterations: 14, Coherence: 0.4525172695340438, Perplexity: -9.071471253067294\\n# Passes: 60, Iterations: 16, Coherence: 0.47780339081003687, Perplexity: -9.11608313556567\\n# Passes: 80, Iterations: 12, Coherence: 0.4611642988790317, Perplexity: -9.060177368492962\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:26:46.659993Z",
     "start_time": "2024-08-04T14:22:07.666487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用最佳参数训练最终LDA模型\n",
    "final_lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20, iterations=16, eval_every=180000)\n",
    "\n",
    "# 输出最终模型的主题\n",
    "for idx, topic in final_lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx}\\nWords: {topic}\\n\")\n",
    "    \n",
    "'''\n",
    "Topic: 0\n",
    "Words: 0.032*\"1\" + 0.027*\"year\" + 0.025*\"2\" + 0.024*\"million\" + 0.022*\"000\" + 0.020*\"billion\" + 0.020*\"animal\" + 0.018*\"3\" + 0.017*\"5\" + 0.014*\"4\"\n",
    "\n",
    "Topic: 1\n",
    "Words: 0.027*\"bushfire\" + 0.027*\"vicfires\" + 0.018*\"info\" + 0.017*\"fire\" + 0.015*\"smoke\" + 0.014*\"south\" + 0.013*\"amp\" + 0.013*\"rain\" + 0.012*\"advice\" + 0.011*\"air\"\n",
    "\n",
    "Topic: 2\n",
    "Words: 0.089*\"australia\" + 0.051*\"koala\" + 0.039*\"australiabushfires\" + 0.036*\"australiaburning\" + 0.035*\"australiaonfire\" + 0.030*\"animal\" + 0.030*\"australiafires\" + 0.028*\"australianbushfiresdisaster\" + 0.020*\"australianfires\" + 0.017*\"rain\"\n",
    "\n",
    "Topic: 3\n",
    "Words: 0.019*\"like\" + 0.016*\"people\" + 0.014*\"know\" + 0.013*\"get\" + 0.012*\"one\" + 0.010*\"look\" + 0.010*\"right\" + 0.010*\"would\" + 0.010*\"think\" + 0.009*\"need\"\n",
    "\n",
    "Topic: 4\n",
    "Words: 0.067*\"auspol\" + 0.032*\"scottmorrisonmp\" + 0.019*\"australiaburns\" + 0.017*\"scottyfrommarketing\" + 0.017*\"government\" + 0.015*\"climateemergency\" + 0.014*\"morrison\" + 0.013*\"amp\" + 0.011*\"australian\" + 0.011*\"australianbushfiredisaster\"\n",
    "\n",
    "Topic: 5\n",
    "Words: 0.063*\"climate\" + 0.041*\"change\" + 0.017*\"bushfireaustralia\" + 0.014*\"amp\" + 0.008*\"climatechange\" + 0.007*\"science\" + 0.007*\"action\" + 0.007*\"fuel\" + 0.006*\"lie\" + 0.006*\"game\"\n",
    "\n",
    "Topic: 6\n",
    "Words: 0.044*\"help\" + 0.021*\"please\" + 0.020*\"australia\" + 0.017*\"donation\" + 0.016*\"australianbushfires\" + 0.014*\"support\" + 0.014*\"donate\" + 0.014*\"australiafires\" + 0.013*\"australianbushfiredisaster\" + 0.013*\"money\"\n",
    "\n",
    "Topic: 7\n",
    "Words: 0.021*\"thank\" + 0.019*\"firefighter\" + 0.017*\"amp\" + 0.012*\"thanks\" + 0.011*\"home\" + 0.011*\"australianfires\" + 0.011*\"work\" + 0.010*\"volunteer\" + 0.009*\"amazing\" + 0.009*\"people\"\n",
    "\n",
    "Topic: 8\n",
    "Words: 0.044*\"australia\" + 0.026*\"fire\" + 0.017*\"climatechange\" + 0.016*\"australiafires\" + 0.014*\"world\" + 0.014*\"bushfires\" + 0.013*\"climateemergency\" + 0.013*\"australianfires\" + 0.012*\"australianbushfiredisaster\" + 0.011*\"climatecrisis\"\n",
    "\n",
    "Topic: 9\n",
    "Words: 0.037*\"fire\" + 0.015*\"power\" + 0.015*\"nsw\" + 0.014*\"community\" + 0.013*\"water\" + 0.012*\"bushfire\" + 0.011*\"affected\" + 0.011*\"island\" + 0.010*\"service\" + 0.010*\"area\"\n",
    "'''"
   ],
   "id": "285c5eb349ec0d66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.025*\"smoke\" + 0.025*\"bushfiresaustralia\" + 0.024*\"bushfires\" + 0.018*\"bushfirecrisisaustralia\" + 0.018*\"melbourne\" + 0.017*\"australianfires\" + 0.016*\"australia\" + 0.015*\"new\" + 0.014*\"nswfires\" + 0.014*\"australiafires\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.061*\"auspol\" + 0.028*\"scottmorrisonmp\" + 0.022*\"scottyfrommarketing\" + 0.020*\"australiaburns\" + 0.018*\"morrison\" + 0.016*\"australianbushfiredisaster\" + 0.012*\"pm\" + 0.010*\"minister\" + 0.009*\"climateemergency\" + 0.009*\"scott\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.077*\"australia\" + 0.030*\"australiafires\" + 0.028*\"australiaonfire\" + 0.026*\"australiaburning\" + 0.024*\"australianbushfiresdisaster\" + 0.021*\"australiabushfires\" + 0.020*\"fire\" + 0.020*\"australianfires\" + 0.019*\"koala\" + 0.018*\"rain\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.030*\"climate\" + 0.019*\"change\" + 0.018*\"amp\" + 0.018*\"climatechange\" + 0.014*\"australia\" + 0.014*\"auspol\" + 0.012*\"climateemergency\" + 0.011*\"climatecrisis\" + 0.010*\"government\" + 0.009*\"australianbushfiredisaster\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.048*\"info\" + 0.045*\"bushfire\" + 0.042*\"vicfires\" + 0.031*\"amp\" + 0.031*\"watch\" + 0.027*\"advice\" + 0.019*\"act\" + 0.011*\"2020\" + 0.007*\"rainfall\" + 0.007*\"conservation\"\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.031*\"help\" + 0.017*\"support\" + 0.016*\"australianbushfiredisaster\" + 0.016*\"please\" + 0.014*\"donation\" + 0.013*\"australianbushfires\" + 0.013*\"australia\" + 0.012*\"money\" + 0.011*\"donate\" + 0.011*\"australiafires\"\n",
      "\n",
      "Topic: 6\n",
      "Words: 0.045*\"fire\" + 0.015*\"nsw\" + 0.012*\"area\" + 0.012*\"south\" + 0.010*\"bushfire\" + 0.010*\"power\" + 0.010*\"amp\" + 0.009*\"nswfires\" + 0.008*\"storm\" + 0.008*\"rain\"\n",
      "\n",
      "Topic: 7\n",
      "Words: 0.039*\"news\" + 0.019*\"air\" + 0.014*\"good\" + 0.013*\"quality\" + 0.012*\"water\" + 0.011*\"heavy\" + 0.009*\"child\" + 0.009*\"fundraising\" + 0.009*\"canberra\" + 0.008*\"house\"\n",
      "\n",
      "Topic: 8\n",
      "Words: 0.019*\"like\" + 0.015*\"day\" + 0.014*\"one\" + 0.014*\"bushfireaustralia\" + 0.014*\"get\" + 0.013*\"look\" + 0.012*\"well\" + 0.012*\"year\" + 0.012*\"last\" + 0.011*\"time\"\n",
      "\n",
      "Topic: 9\n",
      "Words: 0.030*\"animal\" + 0.023*\"australia\" + 0.018*\"fire\" + 0.018*\"koala\" + 0.016*\"1\" + 0.015*\"million\" + 0.015*\"wildlife\" + 0.014*\"000\" + 0.013*\"billion\" + 0.012*\"home\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTopic: 0\\nWords: 0.032*\"1\" + 0.027*\"year\" + 0.025*\"2\" + 0.024*\"million\" + 0.022*\"000\" + 0.020*\"billion\" + 0.020*\"animal\" + 0.018*\"3\" + 0.017*\"5\" + 0.014*\"4\"\\n\\nTopic: 1\\nWords: 0.027*\"bushfire\" + 0.027*\"vicfires\" + 0.018*\"info\" + 0.017*\"fire\" + 0.015*\"smoke\" + 0.014*\"south\" + 0.013*\"amp\" + 0.013*\"rain\" + 0.012*\"advice\" + 0.011*\"air\"\\n\\nTopic: 2\\nWords: 0.089*\"australia\" + 0.051*\"koala\" + 0.039*\"australiabushfires\" + 0.036*\"australiaburning\" + 0.035*\"australiaonfire\" + 0.030*\"animal\" + 0.030*\"australiafires\" + 0.028*\"australianbushfiresdisaster\" + 0.020*\"australianfires\" + 0.017*\"rain\"\\n\\nTopic: 3\\nWords: 0.019*\"like\" + 0.016*\"people\" + 0.014*\"know\" + 0.013*\"get\" + 0.012*\"one\" + 0.010*\"look\" + 0.010*\"right\" + 0.010*\"would\" + 0.010*\"think\" + 0.009*\"need\"\\n\\nTopic: 4\\nWords: 0.067*\"auspol\" + 0.032*\"scottmorrisonmp\" + 0.019*\"australiaburns\" + 0.017*\"scottyfrommarketing\" + 0.017*\"government\" + 0.015*\"climateemergency\" + 0.014*\"morrison\" + 0.013*\"amp\" + 0.011*\"australian\" + 0.011*\"australianbushfiredisaster\"\\n\\nTopic: 5\\nWords: 0.063*\"climate\" + 0.041*\"change\" + 0.017*\"bushfireaustralia\" + 0.014*\"amp\" + 0.008*\"climatechange\" + 0.007*\"science\" + 0.007*\"action\" + 0.007*\"fuel\" + 0.006*\"lie\" + 0.006*\"game\"\\n\\nTopic: 6\\nWords: 0.044*\"help\" + 0.021*\"please\" + 0.020*\"australia\" + 0.017*\"donation\" + 0.016*\"australianbushfires\" + 0.014*\"support\" + 0.014*\"donate\" + 0.014*\"australiafires\" + 0.013*\"australianbushfiredisaster\" + 0.013*\"money\"\\n\\nTopic: 7\\nWords: 0.021*\"thank\" + 0.019*\"firefighter\" + 0.017*\"amp\" + 0.012*\"thanks\" + 0.011*\"home\" + 0.011*\"australianfires\" + 0.011*\"work\" + 0.010*\"volunteer\" + 0.009*\"amazing\" + 0.009*\"people\"\\n\\nTopic: 8\\nWords: 0.044*\"australia\" + 0.026*\"fire\" + 0.017*\"climatechange\" + 0.016*\"australiafires\" + 0.014*\"world\" + 0.014*\"bushfires\" + 0.013*\"climateemergency\" + 0.013*\"australianfires\" + 0.012*\"australianbushfiredisaster\" + 0.011*\"climatecrisis\"\\n\\nTopic: 9\\nWords: 0.037*\"fire\" + 0.015*\"power\" + 0.015*\"nsw\" + 0.014*\"community\" + 0.013*\"water\" + 0.012*\"bushfire\" + 0.011*\"affected\" + 0.011*\"island\" + 0.010*\"service\" + 0.010*\"area\"\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:28:30.346331Z",
     "start_time": "2024-08-04T14:28:05.269920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 输出模型 分配主题到df 输出数据到csv文件\n",
    "# 分配主题 \n",
    "def get_dominant_topic(model, corpus):\n",
    "    dominant_topics = []\n",
    "    for bow in corpus:\n",
    "        topic_probs = model.get_document_topics(bow)\n",
    "        dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
    "        dominant_topics.append(dominant_topic)\n",
    "    return dominant_topics\n",
    "\n",
    "df['dominant_topic'] = get_dominant_topic(final_lda_model, corpus)\n"
   ],
   "id": "5d0ca2d7fde77b03",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:31:21.789597Z",
     "start_time": "2024-08-04T14:31:19.098276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 输出模型\n",
    "final_lda_model.save('../../models/lda_model.gensim')\n",
    "dictionary.save('../../data/processed/dictionary.gensim')\n",
    "corpora.MmCorpus.serialize('../../data/processed/corpus.mm', corpus)\n",
    "\n",
    "# 使用这个语法在jupyter框架外加载模型\n",
    "# final_lda_model = models.LdaModel.load('../../models/lda_model.gensim')\n",
    "# dictionary = corpora.Dictionary.load('../../data/processed/dictionary.gensim')\n",
    "# corpus = corpora.MmCorpus('../../data/processed/corpus.mm')"
   ],
   "id": "e1fef9fff2f5e06d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 输出数据到csv文件\n",
    "df.to_csv('../../data/processed/tweets_with_topics.csv', index=False, encoding='utf-8-sig')"
   ],
   "id": "13259145aa239d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 生成可视化HTML文件以查看关键topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# 准备pyLDAvis数据\n",
    "lda_display = gensimvis.prepare(final_lda_model, corpus, dictionary)\n",
    "\n",
    "# 显示可视化界面\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "# 保存可视化结果到HTML文件\n",
    "pyLDAvis.save_html(lda_display, '../../backend/utils')\n"
   ],
   "id": "44f3e2c055207ea1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 生成词图 有需要时可以使用\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 自定义函数将所有词汇转换为大写\n",
    "def to_upper_case(frequencies):\n",
    "    return {word.upper(): freq for word, freq in frequencies.items()}\n",
    "\n",
    "\n",
    "# 创建一个大图，将所有词云汇聚在一起\n",
    "num_topics = final_lda_model.num_topics\n",
    "fig, axes = plt.subplots(2, (num_topics + 1) // 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "# 生成每个主题的词云并汇聚到一张图中\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < num_topics:\n",
    "        topic_words = dict(final_lda_model.show_topic(i, 200))\n",
    "        topic_words_upper = to_upper_case(topic_words)\n",
    "\n",
    "        wordcloud = WordCloud(width=800, height=600,\n",
    "                              background_color='white',\n",
    "                              max_words=200,\n",
    "                              contour_width=3,\n",
    "                              contour_color='steelblue',\n",
    "                              random_state=21,\n",
    "                              max_font_size=110).generate_from_frequencies(topic_words_upper)\n",
    "\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.set_title(f'Topic {i}', fontsize=16)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "# 调整子图布局\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../backend/utils/LDA_wordcloud.png', format='png', bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "740a255838b0f31d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 从LDA_visualization.html文件 和LDA_wordcloud.png中 总结出以下和山火相关新闻关键词\n",
    "# 使用这些关键词将推文进行过滤 保留与山火相关的推文 保存为tweets_bushfire_related_keywords.csv\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv('../../data/processed/tweets_with_topics.csv')\n",
    "\n",
    "# From HTML file\n",
    "bushfire_keywords = [ \n",
    "    \"bushfiredisaster\", \"australfires\", \"fire\", \"australianfires\", \"bushfires\",\n",
    "    \"australiaburns\", \"australianbushfiredisaster\", \"australianbushfires\",\n",
    "    \"australfire\", \"australiaburning\", \"koala\", \"animal\", \"australianwildfires\",\n",
    "    \"australianbushfire\", \"firefighter\", \"bushfireaustralia\", \"nswfires\", \"vicfires\",\n",
    "    \"bushfirecrisis\", \"bushfiresaustralia\", \"bushfirecrisisaustralia\"\n",
    "]\n",
    "\n",
    "# 使用关键词过滤推文\n",
    "df_bushfire_related = df[\n",
    "    df['text'].str.contains('|'.join(bushfire_keywords), case=False, na=False)\n",
    "]\n",
    "# 将过滤后的数据保存到新的CSV文件\n",
    "output_path = '../../data/processed/tweets_bushfire_related_keywords.csv'\n",
    "df_bushfire_related.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(len(df_bushfire_related))  # 158902\n"
   ],
   "id": "718d14a4d0223f38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T14:27:17.689201Z",
     "start_time": "2024-08-04T14:27:14.564891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 聚合location列统计每个地点的推文数   ===== 地点 =====   这段后期有需要再用\n",
    "location_counts = df.groupby('location').size()\n",
    "location_counts = location_counts.sort_values(ascending=False)\n",
    "print(location_counts.head(10))"
   ],
   "id": "ec44aa84d0cf5933",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "Unknown                       75217\n",
      "Victoria, Australia           18388\n",
      "New South Wales, Australia    18228\n",
      "United States                 15909\n",
      "Australia                     15617\n",
      "United Kingdom                 8312\n",
      "Queensland, Australia          6027\n",
      "South Australia, Australia     4074\n",
      "Canada                         3704\n",
      "India                          3492\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
