{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:45:58.890584Z",
     "start_time": "2024-10-07T12:35:01.876997Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.utils.process import abbrev_cwd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "df = pd.read_csv('labeled_news_dataset_9.csv')\n",
    "\n",
    "# Select necessary columns\n",
    "df = df[['_id', 'author','hashtags','verified', 'sentiment', 'cleaned_text', 'fake_news_tag']].dropna()\n",
    "\n",
    "# Rename columns for consistency\n",
    "df.rename(columns={'cleaned_text': 'text', 'fake_news_tag': 'label'}, inplace=True)\n",
    "\n",
    "# Remove duplicates based on text\n",
    "df.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_labeled = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Display class distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(df_labeled['label'].value_counts())\n",
    "\n",
    "texts = df_labeled['text'].tolist()\n",
    "labels = df_labeled['label'].tolist()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"  # Using DistilBERT for efficiency\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize training data\n",
    "train_encodings = tokenizer(\n",
    "    X_train,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# Tokenize testing data\n",
    "test_encodings = tokenizer(\n",
    "    X_test,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "\n",
    "class FakeNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = FakeNewsDataset(train_encodings, y_train)\n",
    "test_dataset = FakeNewsDataset(test_encodings, y_test)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "# Define metric calculation\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    return {\n",
    "        'accuracy': (preds == labels).mean()\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "0    850\n",
      "1    593\n",
      "Name: label, dtype: int64\n",
      "Training samples: 1154, Test samples: 289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\Python\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "D:\\Python\\Lib\\site-packages\\transformers\\training_args.py:1540: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='291' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [291/291 10:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.265369</td>\n",
       "      <td>0.916955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>0.224082</td>\n",
       "      <td>0.923875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.264021</td>\n",
       "      <td>0.920415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=291, training_loss=0.2949536610490729, metrics={'train_runtime': 651.5546, 'train_samples_per_second': 5.313, 'train_steps_per_second': 0.447, 'total_flos': 66282339700944.0, 'train_loss': 0.2949536610490729, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T14:25:07.528638Z",
     "start_time": "2024-10-08T14:25:05.892716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained model\n",
    "trainer.save_model('./trained_model_v3')\n",
    "\n",
    "df_full = pd.read_csv('../../data/processed/tweets_with_sentiment_vader.csv')\n",
    "\n",
    "# Ensure necessary columns are present and drop NaNs\n",
    "df_full = df_full[['_id', 'cleaned_text']].dropna()\n",
    "\n",
    "print(f\"Total tweets to predict: {len(df_full)}\")\n",
    "\n",
    "# Prepare the texts for prediction\n",
    "texts_full = df_full['cleaned_text'].tolist()\n",
    "\n",
    "# Tokenize the full dataset\n",
    "encodings_full = tokenizer(\n",
    "    texts_full,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# Create a dataset object for the full dataset\n",
    "class FullDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "full_dataset = FullDataset(encodings_full)\n",
    "\n",
    "# Create a DataLoader for the full dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(full_dataset, batch_size=12)\n",
    "\n",
    "# Load the trained model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./trained_model_v3', num_labels=2)\n",
    "model.eval()\n",
    "\n",
    "# Ensure the model is on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to store predictions and probabilities\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "# Run predictions on the full dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Get the probabilities for the 'fake news' class (assuming label '1' is fake news)\n",
    "        probs_fake = probs[:, 1].cpu().numpy()\n",
    "\n",
    "        # Get predicted labels\n",
    "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "        # Append results to the lists\n",
    "        all_preds.extend(preds)\n",
    "        all_probs.extend(probs_fake)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "df_full['fake_news_pred'] = all_preds\n",
    "df_full['fake_news_prob'] = all_probs\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df_full.to_csv('tweets_with_fake_news_predictions_final.csv', index=False)\n",
    "print(\"Predictions saved to 'tweets_with_fake_news_predictions_final.csv'\")"
   ],
   "id": "5805a5032db943fa",
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"ËØ∑Ê±ÇÁöÑÊìç‰ΩúÊó†Ê≥ïÂú®‰ΩøÁî®Áî®Êà∑Êò†Â∞ÑÂå∫ÂüüÊâìÂºÄÁöÑÊñá‰ª∂‰∏äÊâßË°å„ÄÇ\" })",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mSafetensorError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Save the trained model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./trained_model_v3\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m df_full \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../data/processed/tweets_with_sentiment_vader.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Ensure necessary columns are present and drop NaNs\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python\\Lib\\site-packages\\transformers\\trainer.py:3454\u001B[0m, in \u001B[0;36mTrainer.save_model\u001B[1;34m(self, output_dir, _internal_call)\u001B[0m\n\u001B[0;32m   3451\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped\u001B[38;5;241m.\u001B[39msave_checkpoint(output_dir)\n\u001B[0;32m   3453\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[1;32m-> 3454\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3456\u001B[0m \u001B[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001B[39;00m\n\u001B[0;32m   3457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpush_to_hub \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _internal_call:\n",
      "File \u001B[1;32mD:\\Python\\Lib\\site-packages\\transformers\\trainer.py:3525\u001B[0m, in \u001B[0;36mTrainer._save\u001B[1;34m(self, output_dir, state_dict)\u001B[0m\n\u001B[0;32m   3523\u001B[0m             torch\u001B[38;5;241m.\u001B[39msave(state_dict, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_dir, WEIGHTS_NAME))\n\u001B[0;32m   3524\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3525\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3526\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe_serialization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_safetensors\u001B[49m\n\u001B[0;32m   3527\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3529\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3530\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(output_dir)\n",
      "File \u001B[1;32mD:\\Python\\Lib\\site-packages\\transformers\\modeling_utils.py:2793\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[0;32m   2788\u001B[0m     gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m   2790\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m safe_serialization:\n\u001B[0;32m   2791\u001B[0m     \u001B[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001B[39;00m\n\u001B[0;32m   2792\u001B[0m     \u001B[38;5;66;03m# joyfulness), but for now this enough.\u001B[39;00m\n\u001B[1;32m-> 2793\u001B[0m     \u001B[43msafe_save_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mformat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2794\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2795\u001B[0m     save_function(shard, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, shard_file))\n",
      "File \u001B[1;32mD:\\Python\\Lib\\site-packages\\safetensors\\torch.py:286\u001B[0m, in \u001B[0;36msave_file\u001B[1;34m(tensors, filename, metadata)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_file\u001B[39m(\n\u001B[0;32m    256\u001B[0m     tensors: Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor],\n\u001B[0;32m    257\u001B[0m     filename: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[0;32m    258\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    259\u001B[0m ):\n\u001B[0;32m    260\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001B[39;00m\n\u001B[0;32m    262\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 286\u001B[0m     serialize_file(_flatten(tensors), filename, metadata\u001B[38;5;241m=\u001B[39mmetadata)\n",
      "\u001B[1;31mSafetensorError\u001B[0m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"ËØ∑Ê±ÇÁöÑÊìç‰ΩúÊó†Ê≥ïÂú®‰ΩøÁî®Áî®Êà∑Êò†Â∞ÑÂå∫ÂüüÊâìÂºÄÁöÑÊñá‰ª∂‰∏äÊâßË°å„ÄÇ\" })"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T16:38:53.487236Z",
     "start_time": "2024-10-07T16:38:52.014953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the predictions\n",
    "df_full = pd.read_csv('tweets_with_fake_news_predictions_final.csv')\n",
    "\n",
    "# Calculate fake news statistics\n",
    "fake_news_count = df_full['fake_news_pred'].sum()\n",
    "fake_news_percentage = (fake_news_count / len(df_full))\n",
    "\n",
    "fake_news_stats = {\n",
    "    \"Total Entries\": len(df_full),\n",
    "    \"Fake News Count\": fake_news_count,\n",
    "    \"Fake News Percentage\": fake_news_percentage\n",
    "}\n",
    "\n",
    "print(pd.DataFrame([fake_news_stats]))"
   ],
   "id": "5de7d44a06d519e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total Entries  Fake News Count  Fake News Percentage\n",
      "0         158902              559              0.003518\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T16:39:01.037034Z",
     "start_time": "2024-10-07T16:38:59.642966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_full = pd.read_csv('tweets_with_fake_news_predictions.csv')\n",
    "\n",
    "fake_news_count = df_full['fake_news_pred'].sum()\n",
    "print(f\"Total Fake News Count: {fake_news_count}\")\n",
    "\n",
    "highest_prob_fake_news = df_full[df_full['fake_news_pred'] == 1].nlargest(1, 'fake_news_prob')\n",
    "print(f\"Highest Probability Fake News:\\n{highest_prob_fake_news[['cleaned_text', 'fake_news_prob']]}\")\n",
    "\n",
    "lowest_prob_fake_news = df_full[df_full['fake_news_pred'] == 1].nsmallest(1, 'fake_news_prob')\n",
    "print(f\"Lowest Probability Fake News:\\n{lowest_prob_fake_news[['cleaned_text', 'fake_news_prob']]}\")\n",
    "\n",
    "highest_prob_real_news = df_full[df_full['fake_news_pred'] == 0].nlargest(1, 'fake_news_prob')\n",
    "print(f\"Highest Probability Real News (misclassified as fake):\\n{highest_prob_real_news[['cleaned_text', 'fake_news_prob']]}\")\n",
    "\n",
    "lowest_prob_real_news = df_full[df_full['fake_news_pred'] == 0].nsmallest(1, 'fake_news_prob')\n",
    "print(f\"Lowest Probability Real News:\\n{lowest_prob_real_news[['cleaned_text', 'fake_news_prob']]}\")"
   ],
   "id": "6972b4440a2fcc94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fake News Count: 14753\n",
      "Highest Probability Fake News:\n",
      "                                            cleaned_text  fake_news_prob\n",
      "82980  question anyone following along australiafires...        0.999426\n",
      "Lowest Probability Fake News:\n",
      "                                             cleaned_text  fake_news_prob\n",
      "147294  hmmm bushfire donation scottyfommarketing hori...         0.50005\n",
      "Highest Probability Real News (misclassified as fake):\n",
      "                                             cleaned_text  fake_news_prob\n",
      "148868  sound like liarinchief prayersforaustralia aus...        0.499995\n",
      "Lowest Probability Real News:\n",
      "                                            cleaned_text  fake_news_prob\n",
      "60361  firefighter need funding resource fight bushfi...        0.009818\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T16:39:34.345366Z",
     "start_time": "2024-10-07T16:39:33.300597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the predictions CSV file (adjust the file path as needed)\n",
    "df_full = pd.read_csv('tweets_with_fake_news_predictions_final.csv')\n",
    "\n",
    "# Define the intervals for probabilities\n",
    "bin_edges = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Use pandas cut function to categorize the fake news probabilities into bins\n",
    "df_full['probability_bin'] = pd.cut(df_full['fake_news_prob'], bins=bin_edges)\n",
    "\n",
    "# Calculate the frequency for each bin\n",
    "probability_distribution = df_full['probability_bin'].value_counts().sort_index()\n",
    "\n",
    "# Display the distribution in a tabular format\n",
    "print(probability_distribution)\n"
   ],
   "id": "8b32f79543127c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.1]    22648\n",
      "(0.1, 0.2]    80999\n",
      "(0.2, 0.3]    49742\n",
      "(0.3, 0.4]     4916\n",
      "(0.4, 0.5]       38\n",
      "(0.5, 0.6]       12\n",
      "(0.6, 0.7]        7\n",
      "(0.7, 0.8]        9\n",
      "(0.8, 0.9]        7\n",
      "(0.9, 1.0]      524\n",
      "Name: probability_bin, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T06:42:51.365163Z",
     "start_time": "2024-10-09T06:41:46.608963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deliver to front end\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "df_fake_news = pd.read_csv('tweets_with_fake_news_predictions_final.csv')\n",
    "df_original = pd.read_csv('../../data/processed/tweets_with_sentiment_vader.csv')\n",
    "\n",
    "# Combine the dataset based on cleaned_text column\n",
    "df_combined = pd.merge(df_fake_news, df_original, on='cleaned_text')\n",
    "\n",
    "# Topic dict\n",
    "topics = {\n",
    "    1: \"Wildlife and Environmental Impact\",\n",
    "    2: \"Fundraising and Community Support\",\n",
    "    3: \"Political Criticism and Government Response\",\n",
    "    4: \"Climate Change Debate\",\n",
    "    5: \"Emotional and Spiritual Reactions\",\n",
    "    6: \"Geographical and Location-Based Information\",\n",
    "    7: \"Regional Air Quality and Environmental Conditions\",\n",
    "    8: \"Wildlife Devastation\",\n",
    "    9: \"Emergency Information and Public Safety\",\n",
    "    10: \"Health and Mental Well-being\"\n",
    "}\n",
    "\n",
    "# Country dict\n",
    "country_names = {\n",
    "    'AF': 'Afghanistan', 'AL': 'Albania', 'DZ': 'Algeria', 'AS': 'American Samoa', 'AD': 'Andorra',\n",
    "    'AO': 'Angola', 'AI': 'Anguilla', 'AQ': 'Antarctica', 'AG': 'Antigua and Barbuda', 'AR': 'Argentina',\n",
    "    'AM': 'Armenia', 'AW': 'Aruba', 'AT': 'Austria', 'AZ': 'Azerbaijan', 'BS': 'Bahamas',\n",
    "    'BH': 'Bahrain', 'BD': 'Bangladesh', 'BB': 'Barbados', 'BY': 'Belarus', 'BE': 'Belgium', 'BZ': 'Belize',\n",
    "    'BJ': 'Benin', 'BM': 'Bermuda', 'BT': 'Bhutan', 'BO': 'Bolivia', 'BA': 'Bosnia and Herzegovina',\n",
    "    'BW': 'Botswana', 'BR': 'Brazil', 'IO': 'British Indian Ocean Territory', 'BN': 'Brunei Darussalam',\n",
    "    'BG': 'Bulgaria', 'BF': 'Burkina Faso', 'BI': 'Burundi', 'KH': 'Cambodia', 'CM': 'Cameroon', 'CA': 'Canada',\n",
    "    'CV': 'Cape Verde', 'KY': 'Cayman Islands', 'CF': 'Central African Republic', 'TD': 'Chad', 'CL': 'Chile',\n",
    "    'CN': 'China', 'CX': 'Christmas Island', 'CC': 'Cocos (Keeling) Islands', 'CO': 'Colombia', 'KM': 'Comoros',\n",
    "    'CG': 'Congo', 'CD': 'Congo, Democratic Republic', 'CK': 'Cook Islands', 'CR': 'Costa Rica', \n",
    "    'CI': \"C√¥te d'Ivoire\", 'HR': 'Croatia', 'CU': 'Cuba', 'CY': 'Cyprus', 'CZ': 'Czech Republic',\n",
    "    'DK': 'Denmark', 'DJ': 'Djibouti', 'DM': 'Dominica', 'DO': 'Dominican Republic', 'EC': 'Ecuador', \n",
    "    'EG': 'Egypt', 'SV': 'El Salvador', 'GQ': 'Equatorial Guinea', 'ER': 'Eritrea', 'EE': 'Estonia', \n",
    "    'ET': 'Ethiopia', 'FK': 'Falkland Islands', 'FO': 'Faroe Islands', 'FJ': 'Fiji', 'FI': 'Finland', 'FR': 'France',\n",
    "    'GF': 'French Guiana', 'PF': 'French Polynesia', 'GA': 'Gabon', 'GM': 'Gambia', 'GE': 'Georgia', 'DE': 'Germany',\n",
    "    'GH': 'Ghana', 'GI': 'Gibraltar', 'GR': 'Greece', 'GL': 'Greenland', 'GD': 'Grenada', 'GP': 'Guadeloupe', \n",
    "    'GU': 'Guam', 'GT': 'Guatemala', 'GG': 'Guernsey', 'GN': 'Guinea', 'GW': 'Guinea-Bissau', 'GY': 'Guyana', \n",
    "    'HT': 'Haiti', 'VA': 'Holy See (Vatican City State)', 'HN': 'Honduras', 'HK': 'Hong Kong', 'HU': 'Hungary', \n",
    "    'IS': 'Iceland', 'IN': 'India', 'ID': 'Indonesia', 'IR': 'Iran', 'IQ': 'Iraq', 'IE': 'Ireland', 'IM': 'Isle of Man',\n",
    "    'IL': 'Israel', 'IT': 'Italy', 'JM': 'Jamaica', 'JP': 'Japan', 'JE': 'Jersey', 'JO': 'Jordan', 'KZ': 'Kazakhstan',\n",
    "    'KE': 'Kenya', 'KI': 'Kiribati', 'KR': 'South Korea', 'KW': 'Kuwait', 'KG': 'Kyrgyzstan', 'LA': 'Laos',\n",
    "    'LV': 'Latvia', 'LB': 'Lebanon', 'LS': 'Lesotho', 'LR': 'Liberia', 'LY': 'Libya', 'LI': 'Liechtenstein', \n",
    "    'LT': 'Lithuania', 'LU': 'Luxembourg', 'MO': 'Macao', 'MK': 'North Macedonia', 'MG': 'Madagascar', \n",
    "    'MW': 'Malawi', 'MY': 'Malaysia', 'MV': 'Maldives', 'ML': 'Mali', 'MT': 'Malta', 'MH': 'Marshall Islands',\n",
    "    'MQ': 'Martinique', 'MR': 'Mauritania', 'MU': 'Mauritius', 'YT': 'Mayotte', 'MX': 'Mexico', 'FM': 'Micronesia',\n",
    "    'MD': 'Moldova', 'MC': 'Monaco', 'MN': 'Mongolia', 'ME': 'Montenegro', 'MS': 'Montserrat', 'MA': 'Morocco', \n",
    "    'MZ': 'Mozambique', 'MM': 'Myanmar', 'NA': 'Namibia', 'NR': 'Nauru', 'NP': 'Nepal', 'NL': 'Netherlands', \n",
    "    'NC': 'New Caledonia', 'NZ': 'New Zealand', 'NI': 'Nicaragua', 'NE': 'Niger', 'NG': 'Nigeria', 'NU': 'Niue', \n",
    "    'NF': 'Norfolk Island', 'MP': 'Northern Mariana Islands', 'NO': 'Norway', 'OM': 'Oman', 'PK': 'Pakistan', \n",
    "    'PW': 'Palau', 'PS': 'Palestine', 'PA': 'Panama', 'PG': 'Papua New Guinea', 'PY': 'Paraguay', 'PE': 'Peru', \n",
    "    'PH': 'Philippines', 'PL': 'Poland', 'PT': 'Portugal', 'PR': 'Puerto Rico', 'QA': 'Qatar', 'RO': 'Romania',\n",
    "    'RU': 'Russia', 'RW': 'Rwanda', 'WS': 'Samoa', 'SM': 'San Marino', 'ST': 'Sao Tome and Principe', 'SA': 'Saudi Arabia',\n",
    "    'SN': 'Senegal', 'RS': 'Serbia', 'SC': 'Seychelles', 'SL': 'Sierra Leone', 'SG': 'Singapore', 'SK': 'Slovakia',\n",
    "    'SI': 'Slovenia', 'SB': 'Solomon Islands', 'SO': 'Somalia', 'ZA': 'South Africa', 'ES': 'Spain', 'LK': 'Sri Lanka', \n",
    "    'SD': 'Sudan', 'SR': 'Suriname', 'SE': 'Sweden', 'CH': 'Switzerland', 'SY': 'Syria', 'TW': 'Taiwan', 'TJ': 'Tajikistan',\n",
    "    'TZ': 'Tanzania', 'TH': 'Thailand', 'TL': 'Timor-Leste', 'TG': 'Togo', 'TK': 'Tokelau', 'TO': 'Tonga', \n",
    "    'TT': 'Trinidad and Tobago', 'TN': 'Tunisia', 'TR': 'Turkey', 'TM': 'Turkmenistan', 'TC': 'Turks and Caicos Islands', \n",
    "    'TV': 'Tuvalu', 'UG': 'Uganda', 'UA': 'Ukraine', 'AE': 'United Arab Emirates', 'GB': 'United Kingdom', 'US': 'United States', \n",
    "    'UY': 'Uruguay', 'UZ': 'Uzbekistan', 'VU': 'Vanuatu', 'VE': 'Venezuela', 'VN': 'Vietnam', 'WF': 'Wallis and Futuna', \n",
    "    'EH': 'Western Sahara', 'YE': 'Yemen', 'ZM': 'Zambia', 'ZW': 'Zimbabwe',\n",
    "    # Australia States and Territories (AU_STATES and AU_ALL are for front end calculation)\n",
    "    'AU-NSW': 'New South Wales, Australia', 'AU-VIC': 'Victoria, Australia', 'AU-QLD': 'Queensland, Australia', \n",
    "    'AU-SA': 'South Australia, Australia', 'AU-WA': 'Western Australia, Australia', 'AU-TAS': 'Tasmania, Australia',\n",
    "    'AU-NT': 'Northern Territory, Australia', 'AU-ACT': 'Australian Capital Territory, Australia'\n",
    "    , 'AU':  'Australia'\n",
    "}\n",
    "\n",
    "# Find the region code based on location column\n",
    "def map_region_code(location):\n",
    "    country_code = location.split(',')[-1].strip()\n",
    "    region_code = [code for code, name in country_names.items() if name == location]\n",
    "    return region_code[0] if region_code else 'Unknown'\n",
    "\n",
    "df_combined['region_code'] = df_combined['location'].apply(map_region_code)\n",
    "df_combined['region_name'] = df_combined['location']\n",
    "\n",
    "# Remove the time section and keep only the date section\n",
    "df_combined['created_at'] = pd.to_datetime(df_combined['created_at']).dt.date\n",
    "\n",
    "# Aggregate data by date and region\n",
    "grouped_data = df_combined.groupby(['created_at', 'region_code', 'region_name']).agg(\n",
    "    tweet_count=('cleaned_text', 'count'),\n",
    "    fake_news_count=('fake_news_pred', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the percentage of fake news\n",
    "grouped_data['fake_news_ratio'] = (grouped_data['fake_news_count'] / grouped_data['tweet_count']) * 100\n",
    "\n",
    "# Map the topic number to the corresponding name\n",
    "df_combined['dominant_topic'] = df_combined['dominant_topic'].apply(lambda t: topics.get(t, \"Unknown topic\"))\n",
    "\n",
    "# Create a new column for identifying the fake news topic\n",
    "fake_news_topics = df_combined.groupby(['created_at', 'region_code', 'region_name'])['dominant_topic'].apply(lambda x: list(x.unique())).reset_index(name='fake_news_topics')\n",
    "grouped_data = pd.merge(grouped_data, fake_news_topics, on=['created_at', 'region_code', 'region_name'])\n",
    "\n",
    "# Format to .json file\n",
    "output = {\n",
    "    \"metadata\": {\n",
    "        \"start_date\": str(df_combined['created_at'].min()),\n",
    "        \"end_date\": str(df_combined['created_at'].max())\n",
    "    },\n",
    "    \"data\": []\n",
    "}\n",
    "\n",
    "# Cluster by day and format to JSON\n",
    "for date, group in grouped_data.groupby('created_at'):\n",
    "    locations = []\n",
    "    for _, row in group.iterrows():\n",
    "        locations.append({\n",
    "            \"region_code\": row['region_code'],\n",
    "            \"region_name\": row['region_name'],\n",
    "            \"tweet_count\": int(row['tweet_count']),\n",
    "            \"fake_news_count\": int(row['fake_news_count']),\n",
    "            \"fake_news_ratio\": round(row['fake_news_ratio'], 4),\n",
    "            \"fake_news_topics\": row['fake_news_topics']\n",
    "        })\n",
    "    output['data'].append({\n",
    "        \"date\": str(date), \n",
    "        \"locations\": locations\n",
    "    })\n",
    "\n",
    "\n",
    "# Save the file\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.datetime64):\n",
    "        return str(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "with open('fake_news_data.json', 'w') as json_file:\n",
    "    json.dump(output, json_file, indent=4, default=convert_numpy_types)\n",
    "\n",
    "print(\"JSON file saved as 'fake_news_data.json'\")\n"
   ],
   "id": "cdba12ef40647c1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file saved as 'fake_news_data.json'\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T06:43:11.737604Z",
     "start_time": "2024-10-09T06:43:11.674294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the result\n",
    "\n",
    "import json\n",
    "\n",
    "with open('fake_news_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "australia_data = []\n",
    "for entry in data['data']:\n",
    "    locations = [loc for loc in entry['locations'] if loc['region_code'].startswith('AU-')]\n",
    "    if locations:\n",
    "        australia_data.append({\n",
    "            \"date\": entry['date'],\n",
    "            \"locations\": locations\n",
    "        })\n",
    "\n",
    "print(json.dumps(australia_data[:15], indent=4))\n"
   ],
   "id": "59983eb49c0c1090",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"date\": \"2019-05-31\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-TAS\",\n",
      "                \"region_name\": \"Tasmania, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Wildlife and Environmental Impact\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-09-07\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-VIC\",\n",
      "                \"region_name\": \"Victoria, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Wildlife Devastation\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-09-15\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-10-17\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-10-27\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Wildlife Devastation\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-08\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-ACT\",\n",
      "                \"region_name\": \"Australian Capital Territory, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 4,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-09\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-ACT\",\n",
      "                \"region_name\": \"Australian Capital Territory, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Fundraising and Community Support\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 3,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\",\n",
      "                    \"Fundraising and Community Support\",\n",
      "                    \"Emotional and Spiritual Reactions\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-VIC\",\n",
      "                \"region_name\": \"Victoria, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Emergency Information and Public Safety\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-WA\",\n",
      "                \"region_name\": \"Western Australia, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Political Criticism and Government Response\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-10\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-ACT\",\n",
      "                \"region_name\": \"Australian Capital Territory, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 3,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\",\n",
      "                    \"Unknown topic\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-VIC\",\n",
      "                \"region_name\": \"Victoria, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Wildlife Devastation\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-11\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-ACT\",\n",
      "                \"region_name\": \"Australian Capital Territory, Australia\",\n",
      "                \"tweet_count\": 2,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 4,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\",\n",
      "                    \"Wildlife Devastation\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-SA\",\n",
      "                \"region_name\": \"South Australia, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Fundraising and Community Support\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-12\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-ACT\",\n",
      "                \"region_name\": \"Australian Capital Territory, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 2,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\",\n",
      "                    \"Emotional and Spiritual Reactions\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-13\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-QLD\",\n",
      "                \"region_name\": \"Queensland, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"region_code\": \"AU-VIC\",\n",
      "                \"region_name\": \"Victoria, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-14\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 2,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Political Criticism and Government Response\",\n",
      "                    \"Regional Air Quality and Environmental Conditions\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-15\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 2,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-16\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 1,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Political Criticism and Government Response\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"date\": \"2019-11-18\",\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"region_code\": \"AU-NSW\",\n",
      "                \"region_name\": \"New South Wales, Australia\",\n",
      "                \"tweet_count\": 2,\n",
      "                \"fake_news_count\": 0,\n",
      "                \"fake_news_ratio\": 0.0,\n",
      "                \"fake_news_topics\": [\n",
      "                    \"Geographical and Location-Based Information\"\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d486f9df57d3d8ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
